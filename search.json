[
  {
    "objectID": "noisy/vis.html",
    "href": "noisy/vis.html",
    "title": "Visualizations of problem landscapes",
    "section": "",
    "text": "Show plots in −  + columns (click on Dimension/Function/Instance/Visualization type below to show all plots for the chosen category)\n\n\n\n\nDimension\n\n\nFunction\n\n\nInstance\n\n\nVisualization type\n\n\n\n\n\n−\n\n +\n\n\n\n−\n\n +\n\n\n\n−\n\n +"
  },
  {
    "objectID": "noisy/vis.html#plot-explanation",
    "href": "noisy/vis.html#plot-explanation",
    "title": "Visualizations of problem landscapes",
    "section": "Plot explanation",
    "text": "Plot explanation\nThe benchmark function definitions in the next section are accompanied with a number of figures.\n\na (3-D) surface plot, where D=2\na contour plot, where D=2\ntwo projected contour plots, where D=20. Plotted are, starting from the optimum \\mathbf{x}^\\mathrm{opt}, first versus second variable (left) and first versus fifth variable (right).\nsections (f versus x) through the global optimum along the first variable x_1, the second variable x_2, and the all-ones vector. The sections for different dimensions appear\n\nin a non-log plot (above), where the maximum f-value is normalized to one for each single graph.\nin a semi-log plot (middle row)\nin a log-log plot (below) starting close to the global optimum along x_1, -x_1, x_2, -x_2, \\mathbf{1}, and -\\mathbf{1}."
  },
  {
    "objectID": "noisy/vis.html#problem-definition",
    "href": "noisy/vis.html#problem-definition",
    "title": "Visualizations of problem landscapes",
    "section": "Problem definition",
    "text": "Problem definition"
  },
  {
    "objectID": "noisy/vis.html#functions-with-moderate-noise",
    "href": "noisy/vis.html#functions-with-moderate-noise",
    "title": "Visualizations of problem landscapes",
    "section": "Functions with moderate noise",
    "text": "Functions with moderate noise\n\nSphere\nf_\\mathrm{sphere}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant\n\n\nf101: Sphere with moderate gaussian noise\nf_{101}(\\mathbf{x}) = f_{\\mathrm{GN}}({f_\\mathrm{sphere}(\\mathbf{x}),0.01}) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf102: Sphere with moderate uniform noise\nf_{102}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),0.01\\left(0.49 + \\dfrac{1}{D}\\right),0.01}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf103: Sphere with moderate seldom cauchy noise\nf_{103}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),0.01,0.05}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nRosenbrock\nf_\\mathrm{rosenbrock}(\\mathbf{x}) = \\sum_{i = 1}^{D-1} 100\\,\\left(z_i^2 - z_{i+1}\\right)^2 + (z_i-1)^2\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}) + 1\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nSo-called banana function due to its 2-D contour lines as a bent ridge (or valley). In the beginning, the prominent first term of the function definition attracts to the point \\mathbf{z}=\\mathbf{0}. Then, a long bending valley needs to be followed to reach the global optimum. The ridge changes its orientation D-1 times.\n\nin larger dimensions the function has a local optimum with an attraction volume of about 25%\n\n\nf104: Rosenbrock with moderate gaussian noise\nf_{104}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),0.01}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf105: Rosenbrock with moderate uniform noise\nf_{105}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),0.01\\left(0.49 + \\dfrac{1}{D}\\right),0.01}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf106: Rosenbrock with moderate seldom cauchy noise\nf_{106}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),0.01,0.05}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}"
  },
  {
    "objectID": "noisy/vis.html#functions-with-severe-noise",
    "href": "noisy/vis.html#functions-with-severe-noise",
    "title": "Visualizations of problem landscapes",
    "section": "Functions with severe noise",
    "text": "Functions with severe noise\n\nSphere\nf_\\mathrm{sphere}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant\n\n\nf107: Sphere with gaussian noise\nf_{107}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf108: Sphere with uniform noise\nf_{108}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf109: Sphere with seldom cauchy noise\nf_{109}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nRosenbrock\nf_\\mathrm{rosenbrock}(\\mathbf{x}) = \\sum_{i = 1}^{D-1} 100\\,\\left(z_i^2 - z_{i+1}\\right)^2 + (z_i-1)^2\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}) + 1\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nSo-called banana function due to its 2-D contour lines as a bent ridge (or valley). In the beginning, the prominent first term of the function definition attracts to the point \\mathbf{z}=\\mathbf{0}. Then, a long bending valley needs to be followed to reach the global optimum. The ridge changes its orientation D-1 times.\n\na local optimum with an attraction volume of about 25%\n\n\nf110: Rosenbrock with gaussian noise\nf_{110}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf111: Rosenbrock with uniform noise\nf_{111}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf112: Rosenbrock with seldom cauchy noise\nf_{112}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nStep ellipsoid\nf_\\mathrm{step}(\\mathbf{x}) = 0.1 \\max\\left(|\\hat{z}_1|/10^4,\\, \\sum_{i = 1}^{D} 10^{2\\frac{i-1}{D-1}} z_i^2\\right)\n\n\\hat{\\mathbf{z}} = \\Lambda^{\\!10}\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\\tilde{z}_i = \\begin{cases}  \\lfloor0.5+\\hat{z}_i\\rfloor & \\text{if~} \\hat{z}_i &gt; 0.5 \\\\  {\\lfloor0.5+10\\,\\hat{z}_i\\rfloor}/{10} & \\text{otherwise}  \\end{cases} for i=1,\\dots,D,\ndenotes the rounding procedure in order to produce the plateaus.\n\\mathbf{z}= \\mathbf{Q}\\tilde{\\mathbf{z}}\n\nProperties:\nThe function consists of many plateaus of different sizes. Apart from a small area close to the global optimum, the gradient is zero almost everywhere.\n\ncondition number is about 100\n\n\nf113: Step ellipsoid with gaussian noise\nf_{113}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{step}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf114: Step ellipsoid with uniform noise\nf_{114}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{step}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf115: Step ellipsoid with seldom cauchy noise\nf_{115}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{step}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nEllipsoid\nf_\\mathrm{ellipsoid}(\\mathbf{x}) = \\sum_{i = 1}^{D} 10^{4\\frac{i-1}{D-1}}z_i^2\n\n\\mathbf{z}= T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nGlobally quadratic ill-conditioned function with smooth local irregularities.\n\ncondition number is 10^4\n\n\nf116: Ellipsoid with gaussian noise\nf_{116}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{ellipsoid}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf117: Ellipsoid with uniform noise\nf_{117}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{ellipsoid}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf118: Ellipsoid with seldom cauchy noise\nf_{118}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{ellipsoid}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nDifferent Powers\nf_\\mathrm{diffpowers}(\\mathbf{x}) = \\sqrt{\\sum_{i = 1}^{D}|z_i|^{2+4\\frac{i - 1}{D- 1}}}\n\n\\mathbf{z}= \\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\nProperties:\nDue to the different exponents the sensitivies of the z_i-variables become more and more different when approaching the optimum.\n\nf119: Different Powers with gaussian noise\nf_{119}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{diffpowers}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf120: Different Powers with uniform noise\nf_{120}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{diffpowers}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf121: Different Powers with seldom cauchy noise\nf_{121}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{diffpowers}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}"
  },
  {
    "objectID": "noisy/vis.html#highly-multi-modal-functions-with-severe-noise",
    "href": "noisy/vis.html#highly-multi-modal-functions-with-severe-noise",
    "title": "Visualizations of problem landscapes",
    "section": "Highly multi-modal functions with severe noise",
    "text": "Highly multi-modal functions with severe noise\n\nSchaffer’s F7\nf_\\mathrm{schaffer}(\\mathbf{x}) = \\left(\\frac{1}{D- 1}\\sum_{i = 1}^{D- 1} \\sqrt{s_i} +\n      \\sqrt{s_i} \\sin^2\\!\\left(50\\,s_i^{1/5}\\right)\\right)^2\n\n\\mathbf{z}= \\Lambda^{\\!10}\\mathbf{Q}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\ns_i = \\sqrt{z_i^2 + z_{i+1}^2} for i=1,\\dots,D\n\nProperties:\nA highly multimodal function where frequency and amplitude of the modulation vary.\n\nconditioning is low\n\n\nf122: Schaffer’s F7 with gaussian noise\nf_{122}(\\mathbf{x}) = f_{\\mathrm{GN}\\left({f_\\mathrm{schaffer}(\\mathbf{x}),1}\\right)} + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf123: Schaffer’s F7 with uniform noise\nf_{123}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{schaffer}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf124: Schaffer’s F7 with seldom cauchy noise\nf_{124}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{schaffer}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nComposite Griewank-Rosenbrock\nf_\\mathrm{f8f2}(\\mathbf{x}) = \\frac{1}{D-1} \\sum_{i=1}^{D-1}\n     \\left(\\frac{s_i}{4000} - \\cos(s_i)\\right) + 1\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)\\mathbf{R}\\mathbf{x}+ 0.5\ns_i = 100\\,(z_i^2 - z_{i+1})^2 + (z_i-1)^2 for i=1,\\dots,D\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nResembling the Rosenbrock function in a highly multimodal way.\n\nf125: Composite Griewank-Rosenbrock with gaussian noise\nf_{125}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{f8f2}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf126: Composite Griewank-Rosenbrock with uniform noise\nf_{126}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{f8f2}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf127: Composite Griewank-Rosenbrock with seldom cauchy noise\nf_{127}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{f8f2}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nGallagher’s Gaussian Peaks, globally rotated\nf_\\mathrm{gallagher}(\\mathbf{x}) = T_\\mathrm{\\hspace*{-0.01emosz}}\\left(10 - \\max_{i=1}^{101}\n                w_i \\exp\\left(-\\frac{1}{2D}\\,\n               (\\mathbf{x}-\\mathbf{y}_i)^{\\mathrm{T}}\\mathbf{R}^{\\mathrm{T}}\n                \\mathbf{C}_i \\mathbf{R}(\\mathbf{x}-\\mathbf{y}_i) \\right)\\right)^2\n\nw_i = \\begin{cases}  1.1 + 8 \\times\\dfrac{i-2}{99} & \\text{for~} i=2,\\dots,101 \\\\  10 & \\text{for~} i = 1  \\end{cases}, three optima have a value larger than 9\n\\mathbf{C}_i=\\Lambda^{\\!\\alpha_i}/\\alpha_i^{1/4} where \\Lambda^{\\!\\alpha_i} is defined as usual, but with randomly permuted diagonal elements. For i=2,\\dots,101, \\alpha_i is drawn uniformly randomly from the set \\left\\{1000^{2\\frac{j}{99}} ~|~ j =  0,\\dots,99\\right\\} without replacement, and \\alpha_i=1000 for i=1.\nthe local optima \\mathbf{y}_i are uniformly drawn from the domain [-4.9,4.9]^D for i=2,\\dots,101 and \\mathbf{y}_{1}\\in[-4,4]^D. The global optimum is at \\mathbf{x^\\mathrm{opt}}=\\mathbf{y}_1.\n\nProperties:\nThe function consists of 101 optima with position and height being unrelated and randomly chosen.\n\ncondition number around the global optimum is about 30\nsame overall rotation matrix\n\n\nf128: Gallagher’s Gaussian Peaks 101-me with gaussian noise\nf_{128}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{gallagher}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf129: Gallagher’s Gaussian Peaks 101-me with uniform noise\nf_{129}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{gallagher}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf130: Gallagher’s Gaussian Peaks 101-me with seldom cauchy noise\nf_{130}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{gallagher}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}"
  },
  {
    "objectID": "mixed-integer/vis.html",
    "href": "mixed-integer/vis.html",
    "title": "Visualizations of problem landscapes",
    "section": "",
    "text": "Plots\nShow plots in −  + columns (click on Dimension/Function/Instance/Visualization type below to show all plots for the chosen category)\n\n\n\n\nDimension\n\n\nFunction\n\n\nInstance\n\n\nVisualization type\n\n\n\n\n\n−\n\n\n\n\n\n+\n\n\n\n\n−\n\n +\n\n\n\n−\n\n +"
  },
  {
    "objectID": "bbob/vis.html",
    "href": "bbob/vis.html",
    "title": "Visualizations of problem landscapes",
    "section": "",
    "text": "Show plots in −  + columns (click on Dimension/Function/Instance/Visualization type below to show all plots for the chosen category)\n\n\n\n\nDimension\n\n\nFunction\n\n\nInstance\n\n\nVisualization type\n\n\n\n\n\n−\n\n +\n\n\n\n−\n\n +\n\n\n\n−\n\n +"
  },
  {
    "objectID": "bbob/vis.html#plots",
    "href": "bbob/vis.html#plots",
    "title": "Visualizations of problem landscapes",
    "section": "",
    "text": "Show plots in −  + columns (click on Dimension/Function/Instance/Visualization type below to show all plots for the chosen category)\n\n\n\n\nDimension\n\n\nFunction\n\n\nInstance\n\n\nVisualization type\n\n\n\n\n\n−\n\n +\n\n\n\n−\n\n +\n\n\n\n−\n\n +"
  },
  {
    "objectID": "bbob/vis.html#plot-explanation",
    "href": "bbob/vis.html#plot-explanation",
    "title": "Visualizations of problem landscapes",
    "section": "Plot explanation",
    "text": "Plot explanation\nThe benchmark function definitions in the next section are accompanied with a number of figures.\n\na (3-D) surface plot, where D=2\na contour plot, where D=2\ntwo projected contour plots, where D=20. Plotted are, starting from the optimum \\mathbf{x}^\\mathrm{opt}, first versus second variable (left) and first versus fifth variable (right).\nsections (f versus x) through the global optimum along the first variable x_1, the second variable x_2, and the all-ones vector. The sections for different dimensions appear\n\nin a non-log plot (above), where the maximum f-value is normalized to one for each single graph.\nin a semi-log plot (middle row)\nin a log-log plot (below) starting close to the global optimum along x _1, -x_1, x_2, -x_2, \\mathbf{1}, and -\\mathbf{1}."
  },
  {
    "objectID": "bbob/vis.html#problem-definition",
    "href": "bbob/vis.html#problem-definition",
    "title": "Visualizations of problem landscapes",
    "section": "Problem definition",
    "text": "Problem definition\n\n\nf1: Sphere Function\nf_{1}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant, scale invariant\n\nInformation gained from this function:\n\nWhat is the optimal convergence rate of an algorithm?\n\n\n\n\n\nf2: Ellipsoidal Function\nf_{2}(\\mathbf{x}) = \\sum_{i = 1}^{D} 10^{6\\frac{i-1}{D-1}}\\,z_i^2 + f_\\mathrm{opt}\\\\\n\n\\mathbf{z}= T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\nProperties:\nGlobally quadratic and ill-conditioned function with smooth local irregularities.\n\nunimodal\nconditioning is about 10^6\n\nInformation gained from this function:\n\nIn comparison to f10: Is separability exploited?\n\n\n\n\n\nf3: Rastrigin Function\nf_{3}(\\mathbf{x}) = 10 \\left(D- \\sum_{i = 1}^{D}\\cos(2\\pi z_i)\\right) + \\|\\mathbf{z}\\|^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\Lambda^{\\!10}T^{{0.2}}_{\\mathrm{asy}}(T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nHighly multimodal function with a comparatively regular structure for the placement of the optima. The transformations T^{{}}_\\mathrm{asy} and T_\\mathrm{\\hspace*{-0.01em}osz} alleviate the symmetry and regularity of the original Rastrigin function\n\nroughly 10^D local optima\nconditioning is about 10\n\nInformation gained from this function:\n\nIn comparison to f15: Is separability exploited?\n\n\n\n\n\nf4: Büche-Rastrigin Function\nf_{4}(\\mathbf{x}) = 10 \\left(D- \\sum_{i = 1}^{D}\\cos(2\\pi z_i)\\right) +\n             \\sum_{i = 1}^{D}z_i^2 + 100\\,f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\nz_i = s_i\\,T_\\mathrm{\\hspace*{-0.01emosz}}(x_i - x_i^\\mathrm{opt}) \\quad \\text{for} \\; i = 1  \\ldots D\ns_i = \\begin{cases}  10\\times10^{\\frac{1}{2}\\,\\frac{i-1}{D-1}} &  \\text{~if~} z_i&gt;0 \\text{~and~} i = 1,3,5,\\ldots  \\\\  10^{\\frac{1}{2}\\frac{i-1}{D-1}} & \\text{~otherwise~}  \\end{cases} for i= 1,\\dots,D\n\nProperties:\nHighly multimodal function with a structured but highly asymmetric placement of the optima. Constructed as a deceptive function for symmetrically distributed search operators.\n\nroughly 10^D local optima, conditioning is about 10, skew factor is about 10 in x-space and 100 in f-space\n\nInformation gained from this function:\n\nIn comparison to f3: What is the effect of asymmetry?\n\n\n\n\n\nf5: Linear Slope\nf_{5}(\\mathbf{x}) = \\sum_{i = 1}^{D} 5\\,|s_i| - s_i z_i  \n   + f_\\mathrm{opt}\n\nz_i = x_i if x_i^\\mathrm{opt}x_i &lt; 5^2 and z_i = x_i^\\mathrm{opt} otherwise, for i=1,\\dots,D. That is, if x_i exceeds x_i^\\mathrm{opt} it will mapped back into the domain and the function appears to be constant in this direction.\ns_i = \\mathrm{{sign}}\\left(x_i^\\mathrm{opt}\\right)\\, 10^{\\frac{i-1}{D-1}} for i=1,\\dots,D.\n\\mathbf{x^\\mathrm{opt}}= \\mathbf{z^\\mathrm{opt}}= 5\\times\\mathbf{1_-^+}\n\nProperties:\nPurely linear function testing whether the search can go outside the initial convex hull of solutions right into the domain boundary.\n\n\\mathbf{x}^\\mathrm{opt} is on the domain boundary\n\nInformation gained from this function:\n\nCan the search go outside the initial convex hull of solutions into the domain boundary? Can the step size be increased accordingly?\n\n\n\n\n\nf6: Attractive Sector Function\nf_{6}(\\mathbf{x}) = T_\\mathrm{\\hspace*{-0.01emosz}}\\left(\\sum_{i = 1}^{D} (s_i z_i)^2\\right)^{0.9} + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{Q}\\Lambda^{\\!10}\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\ns_i = \\begin{cases}  10^2& \\text{if~} z_i\\times x_i^\\mathrm{opt}&gt; 0\\\\  1 & \\text{otherwise}  \\end{cases}\n\nProperties:\nHighly asymmetric function, where only one “hypercone” (with angular base area) with a volume of roughly {1}/{2^D} yields low function values. The optimum is located at the tip of this cone.\n\nunimodal\n\nInformation gained from this function:\n\nIn comparison to f1: What is the effect of a highly asymmetric landscape?\n\n\n\n\n\nf7: Step Ellipsoidal Function\nf_{7}(\\mathbf{x}) = 0.1 \\max\\left(|\\hat{z}_1|/10^4,\\, \\sum_{i = 1}^{D}\n                         10^{2\\frac{i-1}{D-1}} z_i^2\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\\hat{\\mathbf{z}} = \\Lambda^{\\!10}\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\\hat{z_i} = \\begin{cases} \\lfloor0.5+\\hat{z}_i\\rfloor & \\text{if~} \\left|\\hat{z}_i\\right| &gt; 0.5 \\\\ {\\lfloor0.5+10\\,\\hat{z}_i\\rfloor}/{10} & \\text{otherwise} \\end{cases} for i=1,\\dots,D,\ndenotes the rounding procedure in order to produce the plateaus.\n\\mathbf{z}= \\mathbf{Q}\\tilde{\\mathbf{z}}\n\nProperties:\nThe function consists of many plateaus of different sizes. Apart from a small area close to the global optimum, the gradient is zero almost everywhere.\n\nunimodal, non-separable, conditioning is about 100\n\nInformation gained from this function:\n\nDoes the search get stuck on plateaus?\n\n\n\n\n\nf8: Rosenbrock Function, original\nf_{8}(\\mathbf{x}) = \\sum_{i = 1}^{D-1} \\left( 100\\,\\left(z_i^2 - z_{i+1}\\right)^2 + (z_i-1)^2 \\right) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}) + 1\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nSo-called banana function due to its 2-D contour lines as a bent ridge (or valley). In the beginning, the prominent first term of the function definition attracts to the point \\mathbf{z}=\\mathbf{0}. Then, a long bending valley needs to be followed to reach the global optimum. The ridge changes its orientation D-1 times. Exceptionally, here \\mathbf{x^\\mathrm{opt}}\\in[-3,3]^D.\n\ntri-band dependency structure, in larger dimensions the function has a local optimum with an attraction volume of about 25%\n\nInformation gained from this function:\n\nCan the search follow a long path with D-1 changes in the direction?\n\n\n\n\n\nf9: Rosenbrock Function, rotated\nf_{9}(\\mathbf{x}) = \\sum_{i = 1}^{D-1} \\left( 100\\,\\left(z_i^2 - z_{i+1}\\right)^2 + (z_i-1)^2 \\right) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\max\\left(1,\\frac{\\sqrt{D}}{8}\\right)\\mathbf{R}\\mathbf{x}+ \\mathbf{1}/2\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nrotated version of the previously defined Rosenbrock function.\nInformation gained from this function:\n\nIn comparison to f8: Can the search follow a long path with D-1 changes in the direction without exploiting partial separability?\n\n\n\n\n\nf10: Ellipsoidal Function\nf_{10}(\\mathbf{x}) = \\sum_{i = 1}^{D} 10^{6\\frac{i-1}{D-1}}z_i^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nGlobally quadratic ill-conditioned function with smooth local irregularities, non-separable counterpart to f_2.\n\nunimodal, conditioning is 10^6\n\nInformation gained from this function:\n\nIn comparison to f2: What is the effect of rotation (non-separability)?\n\nNote: The 3d plot shows only a part of the complete function in the vicinity of the optimum.\n\n\n\n\nf11: Discus Function\nf_{11}(\\mathbf{x}) = 10^6 z_1^2 + \\sum_{i = 2}^{D} z_i^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nGlobally quadratic function with local irregularities. A single direction in search space is a thousand times more sensitive than all others.\n\nconditioning is about 10^6\n\nInformation gained from this function:\n\nIn comparison to f1: What is the effect of constraint-like penalization?\n\nNote: The 3d plot shows only a part of the complete function in the vicinity of the optimum.\n\n\n\n\nf12: Bent Cigar Function\nf_{12}(\\mathbf{x}) = z_1^2 + 10^6\\sum_{i = 2}^{D} z_i^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{R}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nA ridge defined as \\sum_{i=2}^{D} z_i^2 =0 needs to be followed. The ridge is smooth but very narrow. Due to T^{{1/2}}_\\mathrm{asy} the overall shape deviates remarkably from being quadratic.\n\nconditioning is about 10^6, rotated, unimodal\n\nInformation gained from this function:\n\nCan the search continuously change its search direction?\n\nNote: The 3d plot shows only a part of the complete function in the vicinity of the optimum.\n\n\n\n\nf13: Sharp Ridge Function\nf_{13}(\\mathbf{x}) = z_1^2 + 100\\,\\sqrt{\\sum_{i = 2}^{D} z_i^2} + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{Q}\\Lambda^{\\!10}\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\nProperties:\nAs for the Bent Cigar function, a ridge defined as \\sum_{i=2}^D z_i^2 = 0 must be followed. The ridge is non-differentiable and the gradient is constant when the ridge is approached from any given point. Following the gradient becomes ineffective close to the ridge where the ridge needs to be followed in z_1-direction to its optimum. The necessary change in “search behavior” close to the ridge is difficult to diagnose, because the gradient towards the ridge does not flatten out.\nInformation gained from this function:\n\nIn comparison to f12: What is the effect of non-smoothness, non-differentiabale ridge?\n\n\n\n\n\nf14: Different Powers Function\nf_{14}(\\mathbf{x}) = \\sqrt{\\sum_{i = 1}^{D}|z_i|^{2+4\\frac{i - 1}{D- 1}}} + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\nProperties:\nDue to the different exponents the sensitivies of the z_i-variables become more and more different when approaching the optimum.\n\n\n\n\nf15: Rastrigin Function\nf_{15}(\\mathbf{x}) = 10 \\left(D- \\sum_{i = 1}^{D}\\cos(2\\pi z_i)\\right)\n    + \\|\\mathbf{z}\\|^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{R}\\Lambda^{\\!10}\\mathbf{Q}\\,T^{{0.2}}_\\mathrm{asy}(T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}})))\n\nProperties:\nPrototypical highly multimodal function which has originally a very regular and symmetric structure for the placement of the optima. The transformations T^{{}}_\\mathrm{asy} and T_\\mathrm{\\hspace*{-0.01em}osz} alleviate the symmetry and regularity of the original Rastrigin function.\n\nnon-separable less regular counterpart of f_3\nroughly 10^D local optima\nconditioning is about 10\n\nInformation gained from this function:\n\nin comparison to f3: What is the effect of non-separability for a highly multimodal function?\n\n\n\n\n\nf16: Weierstrass Function\nf_{16}(\\mathbf{x}) = 10 \\left( \\frac{1}{D}\n   \\sum_{i = 1}^{D}\\sum_{k = 0}^{11} 1/2^k \\cos(2\\pi3^k(z_i + 1/2))\n            - f_0 \\right)^3 + \\frac{10}{D}f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{R}\\Lambda^{\\!1/100}\\mathbf{Q}\\,T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\nf_0 = \\sum_{k = 0}^{11} 1/2^k \\cos(2\\pi3^k 1/2)\n\nProperties:\nHighly rugged and moderately repetitive landscape, where the global optimum is not unique.\n\nthe term \\sum_k 1/2^k \\cos(2\\pi3^k\\dots) introduces the ruggedness, where lower frequencies have a larger weight 1/2^k.\nrotated, locally irregular, non-unique global optimum\n\nInformation gained from this function:\n\nin comparison to f17: Does ruggedness or a repetitive landscape deter the search behavior?\n\n\n\n\n\nf17: Schaffers F7 Function\nf_{17}(\\mathbf{x}) = \\left(\\frac{1}{D- 1}\\sum_{i = 1}^{D- 1} \\sqrt{s_i} +\n      \\sqrt{s_i} \\sin^2\\!\\left(50\\,s_i^{1/5}\\right)\\right)^2 + 10\\,f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\Lambda^{\\!10}\\mathbf{Q}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\ns_i = \\sqrt{z_i^2 + z_{i+1}^2} for i=1,\\dots,D\n\nProperties:\nA highly multimodal function where frequency and amplitude of the modulation vary.\n\nasymmetric, rotated\nconditioning is low\n\n\n\n\n\nf18: Schaffers F7 Function, moderately ill-conditioned\nf_{18}(\\mathbf{x}) = \\left(\\frac{1}{D- 1}\\sum_{i = 1}^{D- 1} \\sqrt{s_i} +\n      \\sqrt{s_i} \\sin^2\\!\\left(50\\,s_i^{1/5}\\right)\\right)^2 + 10\\,f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\Lambda^{\\!1000}\\mathbf{Q}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\ns_i = \\sqrt{z_i^2 + z_{i+1}^2} for i=1,\\dots,D\n\nProperties:\nModerately ill-conditioned counterpart to f_{17}\n\nconditioning of about 1000\n\nInformation gained from this function:\n\nIn comparison to f17: What is the effect of ill-conditioning?\n\n\n\n\n\nf19: Composite Griewank-Rosenbrock Function F8F2\nf_{19}(\\mathbf{x}) = \\frac{10}{D-1} \\sum_{i=1}^{D-1} \\left( \\frac{s_i}{4000} - \\cos(s_i) \\right) + 10 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)\\mathbf{R}\\mathbf{x}+ 0.5\ns_i = 100\\,(z_i^2 - z_{i+1})^2 + (z_i-1)^2 for i=1,\\dots,D\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nResembling the Rosenbrock function in a highly multimodal way.\n\n\n\n\nf20: Schwefel Function\nf_{20}(\\mathbf{x}) = - \\frac{1}{100D}%  % kept in the final print\n\\sum_{i = 1}^{D}z_i\\sin(\\sqrt{|z_i|}) + 4.189828872724339\n     + 100f_{\\mathrm{pen}}(\\mathbf{z}/100) + f_\\mathrm{opt}\n\n\\hat{\\mathbf{x}} = 2\\times\\mathbf{1_-^+}\\otimes\\mathbf{x}\n\\hat{z}_{1} = \\hat{x}_{1},\\quad \\hat{z}_{i+1} = \\hat{x}_{i+1} + 0.25 \\left({\\hat{x}_{i}} - 2|x_i^{\\text{opt}}| \\right),\\quad \\text{for } i = 1, \\ldots, D - 1\n\\mathbf{z}= 100 (\\Lambda^{10} (\\hat{\\mathbf{z}} - 2\\left|\\mathbf{x^\\mathrm{opt}}\\right|) + 2\\left|\\mathbf{x^\\mathrm{opt}}\\right|)\n\\mathbf{x^\\mathrm{opt}}= 4.2096874633/2 \\;\\mathbf{1_-^+}, where \\mathbf{1}_-^+ is the same realization as above\n\nProperties:\nThe most prominent 2^D minima are located comparatively close to the corners of the unpenalized search area.\n\nthe penalization is essential, as otherwise more and better minima occur further away from the search space origin\n\n\n\n\n\nf21: Gallagher’s Gaussian 101-me Peaks Function\nf_{21}(\\mathbf{x}) = T_\\mathrm{\\hspace*{-0.01emosz}}\\left( 10 - \\max_{i=1}^{101}\n       w_i \\exp\\left(-\\frac{1}{2D}\\,\n           (\\mathbf{x}-\\mathbf{y}_i)^{\\mathrm{T}}\\mathbf{R}^{\\mathrm{T}}\n           \\mathbf{C}_i \\mathbf{R}(\\mathbf{x}-\\mathbf{y}_i) \\right) \\right)^2 + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\nw_i = \\begin{cases}  1.1 + 8 \\times\\dfrac{i-2}{99} & \\text{for~} i=2,\\dots,101 \\\\  10 & \\text{for~} i = 1  \\end{cases}, three optima have a value larger than 9\n\\mathbf{C}_i=\\Lambda^{\\!\\alpha_i}/\\alpha_i^{1/4} where \\Lambda^{\\!\\alpha_i} is defined as usual, but with randomly permuted diagonal elements. For i=2,\\dots,101, \\alpha_i is drawn uniformly randomly from the set \\left\\{1000^{2\\frac{j}{99}} ~|~ j =  0,\\dots,99\\right\\} without replacement, and \\alpha_i=1000 for i=1.\nthe local optima \\mathbf{y}_i are uniformly drawn from the domain [-5,5]^D for i=2,\\dots,101 and \\mathbf{y}_{1}\\in[-4,4]^D. The global optimum is at \\mathbf{x^\\mathrm{opt}}=\\mathbf{y}_1.\n\nProperties:\nThe function consists of 101 optima with position and height being unrelated and randomly chosen (different for each instantiation of the function).\n\nthe conditioning around the global optimum is about 30\n\nInformation gained from this function:\n\nIs the search effective without any global structure?\n\n\n\n\n\nf22: Gallagher’s Gaussian 21-hi Peaks Function\nf_{22}(\\mathbf{x}) = T_\\mathrm{\\hspace*{-0.01emosz}}\\left( 10 - \\max_{i=1}^{21}\n       w_i \\exp\\left(-\\frac{1}{2D}\\,\n           (\\mathbf{x}-\\mathbf{y}_i)^{\\mathrm{T}}\\mathbf{R}^{\\mathrm{T}}\n           \\mathbf{C}_i \\mathbf{R}(\\mathbf{x}-\\mathbf{y}_i) \\right) \\right)^2 + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\nw_i = \\begin{cases}  1.1 + 8 \\times\\dfrac{i-2}{19} & \\text{for~} i=2,\\dots,21 \\\\  10 & \\text{for~} i = 1  \\end{cases}, two optima have a value larger than 9\n\\mathbf{C}_i=\\Lambda^{\\!\\alpha_i}/\\alpha_i^{1/4} where \\Lambda^{\\!\\alpha_i} is defined as usual, but with randomly permuted diagonal elements. For i=2,\\dots,21, \\alpha_i is drawn uniformly randomly from the set \\left\\{1000^{2\\frac{j}{19}} ~|~ j =  0,\\dots,19\\right\\} without replacement, and \\alpha_i=1000^2 for i=1.\nthe local optima \\mathbf{y}_i are uniformly drawn from the domain [-4.9,4.9]^D for i=2,\\dots,21 and \\mathbf{y}_{1}\\in[[-4,4]^D \\longrightarrow]  [-3.92,3.92]^D]. The global optimum is at \\mathbf{x^\\mathrm{opt}}=\\mathbf{y}_1.\n\nProperties:\nThe function consists of 21 optima with position and height being unrelated and randomly chosen (different for each instantiation of the function).\n\nthe conditioning around the global optimum is about 1000\n\nInformation gained from this function:\n\nIn comparison to f21: What is the effect of higher condition?\n\n\n\n\n\nf23: Katsuura Function\nf_{23}(\\mathbf{x}) = \\frac{10}{D^{2}} \\prod_{i=1}^D\\left(1 + i\\sum_{j=1}^{32}\n     \\frac{\\left|2^j z_i - [2^j z_i]\\right|}{2^j} \\right)^{10/D^{1.2}}\n       - \\frac{10}{D^{2}} + f_{\\mathrm{pen}}(\\mathbf{x}) + f_{\\mathrm{opt}}(\\mathbf{x})\n\n\\mathbf{z}= \\mathbf{Q}\\,\\Lambda^{\\!100}\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}})\n\nProperties:\nHighly rugged and highly repetitive function with more than 10^D global optima.\n\n\n\n\nf24: Lunacek bi-Rastrigin Function\nf_{24}(\\mathbf{x}) = \\mathrm{min}\\left(\\sum_{i = 1}^{D}(\\hat{x}_i - \\mu_0)^2,\n                       d\\,D+ s\\sum_{i = 1}^{D}(\\hat{x}_i - \\mu_1)^2\\right)\n           + 10\\left(D- \\sum_{i=1}^D\\cos(2\\pi z_i)\\right) + 10^4\\,f^{{}_\\mathrm{pen}}(\\mathbf{x}) + f_{\\mathrm{opt}}(\\mathbf{x})\n\n\\hat{\\mathbf{x}} = 2\\,\\mathrm{{sign}}(\\mathbf{x^\\mathrm{opt}})\\otimes\\mathbf{x}, \\mathbf{x^\\mathrm{opt}}= [\\mu_0 \\longrightarrow]  \\frac{\\mu_0}{2} \\mathbf{1_-^+}\n\\mathbf{z}= \\mathbf{Q}\\Lambda^{\\!100}\\mathbf{R}(\\hat{\\mathbf{x}}-\\mu_0\\,\\mathbf{1})\n\\mu_0 = 2.5, \\mu_1 = -\\sqrt{\\dfrac{\\mu_0^2-d}{s}}, s = 1 - \\dfrac{1}{2\\sqrt{D+20}-8.2}, d=1\n\nProperties:\nHighly multimodal function with two funnels around [\\mu_0\\mathbf{1_-^+} \\longrightarrow]  \\frac{\\mu_0}{2}\\mathbf{1_-^+} and [-\\mu_1\\mathbf{1_-^+} \\longrightarrow]  \\frac{\\mu_1}{2}\\mathbf{1_-^+} being superimposed by the cosine. Presumably different approaches need to be used for “selecting the funnel” and for search the highly multimodal function “within” the funnel. The function was constructed to be deceptive for evolutionary algorithms with large population size.\n\nthe funnel of the local optimum at [-\\mu_1 \\mathbf{1_-^+}]  \\frac{\\mu_1}{2}\\mathbf{1_-^+} has roughly 70\\% of the search space volume within [-5,5]^D.\n\nInformation gained from this function: Can the search behavior be local on the global scale but global on a local scale?"
  },
  {
    "objectID": "bbob/def.html",
    "href": "bbob/def.html",
    "title": "Function definitions",
    "section": "",
    "text": "This document is based on the BBOB 2009 function document. In the following, 24 noise-free real-parameter single-objective benchmark functions are presented. Our intention behind the selection of benchmark functions was to evaluate the performance of algorithms with regard to typical difficulties which we believe occur in continuous domain search. We hope that the function collection reflects, at least to a certain extend and with a few exceptions, a more difficult portion of the problem distribution that will be seen in practice (easy functions are evidently of lesser interest).\nWe prefer benchmark functions that are comprehensible such that algorithm behaviours can be understood in the topological context. In this way, a desired search behaviour can be pictured and deficiencies of algorithms can be profoundly analysed. Last but not least, this can eventually lead to a systematic improvement of algorithms.\nAll benchmark functions are scalable with the dimension. Most functions have no specific value of their optimal solution (they are randomly shifted in x-space). All functions have an artificially chosen optimal function value (they are randomly shifted in f-space). Consequently, for each function different instances can be generated: for each instance the randomly chosen values are drawn anew. Apart from the first subgroup, the benchmarks are non-separable. Other specific properties are discussed in the appendix."
  },
  {
    "objectID": "bbob/def.html#introduction",
    "href": "bbob/def.html#introduction",
    "title": "Function definitions",
    "section": "",
    "text": "This document is based on the BBOB 2009 function document. In the following, 24 noise-free real-parameter single-objective benchmark functions are presented. Our intention behind the selection of benchmark functions was to evaluate the performance of algorithms with regard to typical difficulties which we believe occur in continuous domain search. We hope that the function collection reflects, at least to a certain extend and with a few exceptions, a more difficult portion of the problem distribution that will be seen in practice (easy functions are evidently of lesser interest).\nWe prefer benchmark functions that are comprehensible such that algorithm behaviours can be understood in the topological context. In this way, a desired search behaviour can be pictured and deficiencies of algorithms can be profoundly analysed. Last but not least, this can eventually lead to a systematic improvement of algorithms.\nAll benchmark functions are scalable with the dimension. Most functions have no specific value of their optimal solution (they are randomly shifted in x-space). All functions have an artificially chosen optimal function value (they are randomly shifted in f-space). Consequently, for each function different instances can be generated: for each instance the randomly chosen values are drawn anew. Apart from the first subgroup, the benchmarks are non-separable. Other specific properties are discussed in the appendix."
  },
  {
    "objectID": "bbob/def.html#general-setup",
    "href": "bbob/def.html#general-setup",
    "title": "Function definitions",
    "section": "General Setup",
    "text": "General Setup\n\nSearch Space\nAll functions are defined and can be evaluated over \\mathcal{R}^{D}, while the actual search domain is given as [-5,5]^{D}.\n\n\nLocation of the optimal \\mathbf{x}^\\mathrm{opt} and of f_\\mathrm{opt}=f(\\mathbf{x^\\mathrm{opt}})\nAll functions have their global optimum in [-5,5]^{D}. The majority of functions has the global optimum in [-4,4]^{D} and for many of them \\mathbf{x}^\\mathrm{opt} is drawn uniformly from this compact. The value for f_\\mathrm{opt} is drawn from a Cauchy distributed random variable, with zero median and with roughly 50% of the values between -100 and 100. The value is rounded after two decimal places and set to \\pm1000 if its absolute value exceeds 1000. In the function definitions a transformed variable vector \\mathbf{z} is often used instead of the argument \\mathbf{x}. The vector \\mathbf{z} has its optimum in \\mathbf{z^\\mathrm{opt}}=\\mathbf{0}, if not stated otherwise.\n\n\nBoundary Handling\nOn some functions a penalty boundary handling is applied as given with f^{{}}_\\mathrm{pen}.\n\n\nLinear Transformations\nLinear transformations of the search space are applied to derive non-separable functions from separable ones and to control the conditioning of the function.\n\n\nNon-Linear Transformations and Symmetry Breaking\nIn order to make relatively simple, but well understood functions less regular, on some functions non-linear transformations are applied in x- or f-space. Both transformations T_\\mathrm{\\hspace*{-0.01emosz}}:\\mathcal{R}^n\\to\\mathcal{R}^n, n\\in\\{1,D\\}, and T^{{}_\\mathrm{asy}}:\\mathcal{R}^D\\to\\mathcal{R}^D are defined coordinate-wise (see below). They are smooth and have, coordinate-wise, a strictly positive derivative. T_\\mathrm{\\hspace*{-0.01emosz}} is oscillating about the identity, where the oscillation is scale invariant w.r.t. the origin. T^{{}}_\\mathrm{asy} is the identity for negative values. When T^{{}}_\\mathrm{asy} is applied, a portion of 1/2^D of the search space remains untransformed."
  },
  {
    "objectID": "bbob/def.html#symbols-and-definitions",
    "href": "bbob/def.html#symbols-and-definitions",
    "title": "Function definitions",
    "section": "Symbols and Definitions ",
    "text": "Symbols and Definitions \nUsed symbols and definitions of, e.g., auxiliary functions are given in the following. Vectors are typeset in bold and refer to column vectors.\n\\otimes indicates element-wise multiplication of two D-dimensional vectors, \\otimes:\\mathcal{R}^D\\times\\mathcal{R}^D\\to\\mathcal{R}^D,  (\\mathbf{x},\\mathbf{y})\\mapsto\\mathrm{{diag}}(\\mathbf{x})\\times\\mathbf{y}=(x_i\\times y_i)_{i=1,\\dots,D}\n\\|.\\| denotes the Euclidean norm, \\|\\mathbf{x}\\|^2=\\sum_i x_i^2.\n[.] denotes the nearest integer value\n\\mathbf{0} =(0,\\dots,0)^{\\mathrm{T}} all zero vector\n\\mathbf{1} =(1,\\dots,1)^{\\mathrm{T}} all one vector\n\\Lambda^{\\!\\alpha} is a diagonal matrix in D dimensions with the ith diagonal element as \\lambda_{ii} =  \\alpha^{\\frac{1}{2}\\frac{i-1}{D-1}}, for i=1,\\dots,D.\nf^{{}}_\\mathrm{pen} :\\mathcal{R}^D\\to\\mathcal{R}, \\mathbf{x}\\mapsto\\sum_{i=1}^D\\max(0,|x_i| - 5)^2\n\\mathbf{1}_-^+ a D-dimensional vector with entries of -1 or 1 with equal probability independently drawn.\n\\mathbf{Q}, \\mathbf{R} orthogonal (rotation) matrices. For one function in one dimension a different realization for respectively \\mathbf{Q} and \\mathbf{R} is used for each instantiation of the function. Orthogonal matrices are generated from standard normally distributed entries by Gram-Schmidt orthonormalization. Columns and rows of an orthogonal matrix form an orthonormal basis.\n\\mathbf{R} see \\mathbf{Q}\nT^{{\\beta}}_\\mathrm{asy} :\\mathcal{R}^D\\to\\mathcal{R}^D, x_i\\mapsto  \\begin{cases}  x_i^{1+\\beta \\frac{i-1}{D-1}\\sqrt{x_i}} & \\text{~if~} x_i&gt;0\\\\  x_i & \\text{~otherwise~}  \\end{cases}, for i=1,\\dots,D.\nT_\\mathrm{\\hspace*{-0.01em}osz} :\\mathcal{R}^n\\to\\mathcal{R}^n, for any positive integer n (n=1 and n=D are used in the following), maps element-wise x\\mapsto\\mathrm{{sign}}(x)\\exp\\left(\\hat{x} +\n       0.049\\left(\\sin(c_1 \\hat{x}) + \\sin(c_2 \\hat{x})\\right)\\right) with \\hat{x}= \\begin{cases}  \\log(|x|) & \\text{if~} x\\not=0 \\\\  0 & \\text{otherwise}  \\end{cases}, \\mathrm{{sign}}(x) = \\begin{cases} -1 & \\text{if~} x &lt; 0 \\\\  0 & \\text{if~} x = 0 \\\\  1 & \\text{otherwise}  \\end{cases}, c_1 = \\begin{cases}  10 & \\text{if~} x &gt; 0\\\\  5.5 & \\text{otherwise}  \\end{cases} and c_2 = \\begin{cases}  7.9 & \\text{if~} x &gt; 0\\\\  3.1 & \\text{otherwise}  \\end{cases}.\n\\mathbf{x}^\\mathrm{opt} optimal solution vector, such that f(\\mathbf{x^\\mathrm{opt}}) is minimal.\n\n\nf1: Sphere Function\nf_{1}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant, scale invariant\n\nInformation gained from this function:\n\nWhat is the optimal convergence rate of an algorithm?\n\n\n\n\n\nf2: Ellipsoidal Function\nf_{2}(\\mathbf{x}) = \\sum_{i = 1}^{D} 10^{6\\frac{i-1}{D-1}}\\,z_i^2 + f_\\mathrm{opt}\\\\\n\n\\mathbf{z}= T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\nProperties:\nGlobally quadratic and ill-conditioned function with smooth local irregularities.\n\nunimodal\nconditioning is about 10^6\n\nInformation gained from this function:\n\nIn comparison to f10: Is separability exploited?\n\n\n\n\n\nf3: Rastrigin Function\nf_{3}(\\mathbf{x}) = 10 \\left(D- \\sum_{i = 1}^{D}\\cos(2\\pi z_i)\\right) + \\|\\mathbf{z}\\|^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\Lambda^{\\!10}T^{{0.2}}_{\\mathrm{asy}}(T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nHighly multimodal function with a comparatively regular structure for the placement of the optima. The transformations T^{{}}_\\mathrm{asy} and T_\\mathrm{\\hspace*{-0.01em}osz} alleviate the symmetry and regularity of the original Rastrigin function\n\nroughly 10^D local optima\nconditioning is about 10\n\nInformation gained from this function:\n\nIn comparison to f15: Is separability exploited?\n\n\n\n\n\nf4: Büche-Rastrigin Function\nf_{4}(\\mathbf{x}) = 10 \\left(D- \\sum_{i = 1}^{D}\\cos(2\\pi z_i)\\right) +\n             \\sum_{i = 1}^{D}z_i^2 + 100\\,f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\nz_i = s_i\\,T_\\mathrm{\\hspace*{-0.01emosz}}(x_i - x_i^\\mathrm{opt}) \\quad \\text{for} \\; i = 1  \\ldots D\ns_i = \\begin{cases}  10\\times10^{\\frac{1}{2}\\,\\frac{i-1}{D-1}} &  \\text{~if~} z_i&gt;0 \\text{~and~} i = 1,3,5,\\ldots  \\\\  10^{\\frac{1}{2}\\frac{i-1}{D-1}} & \\text{~otherwise~}  \\end{cases} for i= 1,\\dots,D\n\nProperties:\nHighly multimodal function with a structured but highly asymmetric placement of the optima. Constructed as a deceptive function for symmetrically distributed search operators.\n\nroughly 10^D local optima, conditioning is about 10, skew factor is about 10 in x-space and 100 in f-space\n\nInformation gained from this function:\n\nIn comparison to f3: What is the effect of asymmetry?\n\n\n\n\n\nf5: Linear Slope\nf_{5}(\\mathbf{x}) = \\sum_{i = 1}^{D} 5\\,|s_i| - s_i z_i  \n   + f_\\mathrm{opt}\n\nz_i = x_i if x_i^\\mathrm{opt}x_i &lt; 5^2 and z_i = x_i^\\mathrm{opt} otherwise, for i=1,\\dots,D. That is, if x_i exceeds x_i^\\mathrm{opt} it will mapped back into the domain and the function appears to be constant in this direction.\ns_i = \\mathrm{{sign}}\\left(x_i^\\mathrm{opt}\\right)\\, 10^{\\frac{i-1}{D-1}} for i=1,\\dots,D.\n\\mathbf{x^\\mathrm{opt}}= \\mathbf{z^\\mathrm{opt}}= 5\\times\\mathbf{1_-^+}\n\nProperties:\nPurely linear function testing whether the search can go outside the initial convex hull of solutions right into the domain boundary.\n\n\\mathbf{x}^\\mathrm{opt} is on the domain boundary\n\nInformation gained from this function:\n\nCan the search go outside the initial convex hull of solutions into the domain boundary? Can the step size be increased accordingly?\n\n\n\n\n\nf6: Attractive Sector Function\nf_{6}(\\mathbf{x}) = T_\\mathrm{\\hspace*{-0.01emosz}}\\left(\\sum_{i = 1}^{D} (s_i z_i)^2\\right)^{0.9} + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{Q}\\Lambda^{\\!10}\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\ns_i = \\begin{cases}  10^2& \\text{if~} z_i\\times x_i^\\mathrm{opt}&gt; 0\\\\  1 & \\text{otherwise}  \\end{cases}\n\nProperties:\nHighly asymmetric function, where only one “hypercone” (with angular base area) with a volume of roughly {1}/{2^D} yields low function values. The optimum is located at the tip of this cone.\n\nunimodal\n\nInformation gained from this function:\n\nIn comparison to f1: What is the effect of a highly asymmetric landscape?\n\n\n\n\n\nf7: Step Ellipsoidal Function\nf_{7}(\\mathbf{x}) = 0.1 \\max\\left(|\\hat{z}_1|/10^4,\\, \\sum_{i = 1}^{D}\n                         10^{2\\frac{i-1}{D-1}} z_i^2\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\\hat{\\mathbf{z}} = \\Lambda^{\\!10}\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\\hat{z_i} = \\begin{cases} \\lfloor0.5+\\hat{z}_i\\rfloor & \\text{if~} \\left|\\hat{z}_i\\right| &gt; 0.5 \\\\ {\\lfloor0.5+10\\,\\hat{z}_i\\rfloor}/{10} & \\text{otherwise} \\end{cases} for i=1,\\dots,D,\ndenotes the rounding procedure in order to produce the plateaus.\n\\mathbf{z}= \\mathbf{Q}\\tilde{\\mathbf{z}}\n\nProperties:\nThe function consists of many plateaus of different sizes. Apart from a small area close to the global optimum, the gradient is zero almost everywhere.\n\nunimodal, non-separable, conditioning is about 100\n\nInformation gained from this function:\n\nDoes the search get stuck on plateaus?\n\n\n\n\n\nf8: Rosenbrock Function, original\nf_{8}(\\mathbf{x}) = \\sum_{i = 1}^{D-1} \\left( 100\\,\\left(z_i^2 - z_{i+1}\\right)^2 + (z_i-1)^2 \\right) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}) + 1\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nSo-called banana function due to its 2-D contour lines as a bent ridge (or valley). In the beginning, the prominent first term of the function definition attracts to the point \\mathbf{z}=\\mathbf{0}. Then, a long bending valley needs to be followed to reach the global optimum. The ridge changes its orientation D-1 times. Exceptionally, here \\mathbf{x^\\mathrm{opt}}\\in[-3,3]^D.\n\ntri-band dependency structure, in larger dimensions the function has a local optimum with an attraction volume of about 25%\n\nInformation gained from this function:\n\nCan the search follow a long path with D-1 changes in the direction?\n\n\n\n\n\nf9: Rosenbrock Function, rotated\nf_{9}(\\mathbf{x}) = \\sum_{i = 1}^{D-1} \\left( 100\\,\\left(z_i^2 - z_{i+1}\\right)^2 + (z_i-1)^2 \\right) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\max\\left(1,\\frac{\\sqrt{D}}{8}\\right)\\mathbf{R}\\mathbf{x}+ \\mathbf{1}/2\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nrotated version of the previously defined Rosenbrock function.\nInformation gained from this function:\n\nIn comparison to f8: Can the search follow a long path with D-1 changes in the direction without exploiting partial separability?\n\n\n\n\n\nf10: Ellipsoidal Function\nf_{10}(\\mathbf{x}) = \\sum_{i = 1}^{D} 10^{6\\frac{i-1}{D-1}}z_i^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nGlobally quadratic ill-conditioned function with smooth local irregularities, non-separable counterpart to f_2.\n\nunimodal, conditioning is 10^6\n\nInformation gained from this function:\n\nIn comparison to f2: What is the effect of rotation (non-separability)?\n\nNote: The 3d plot shows only a part of the complete function in the vicinity of the optimum.\n\n\n\n\nf11: Discus Function\nf_{11}(\\mathbf{x}) = 10^6 z_1^2 + \\sum_{i = 2}^{D} z_i^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nGlobally quadratic function with local irregularities. A single direction in search space is a thousand times more sensitive than all others.\n\nconditioning is about 10^6\n\nInformation gained from this function:\n\nIn comparison to f1: What is the effect of constraint-like penalization?\n\nNote: The 3d plot shows only a part of the complete function in the vicinity of the optimum.\n\n\n\n\nf12: Bent Cigar Function\nf_{12}(\\mathbf{x}) = z_1^2 + 10^6\\sum_{i = 2}^{D} z_i^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{R}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nA ridge defined as \\sum_{i=2}^{D} z_i^2 =0 needs to be followed. The ridge is smooth but very narrow. Due to T^{{1/2}}_\\mathrm{asy} the overall shape deviates remarkably from being quadratic.\n\nconditioning is about 10^6, rotated, unimodal\n\nInformation gained from this function:\n\nCan the search continuously change its search direction?\n\nNote: The 3d plot shows only a part of the complete function in the vicinity of the optimum.\n\n\n\n\nf13: Sharp Ridge Function\nf_{13}(\\mathbf{x}) = z_1^2 + 100\\,\\sqrt{\\sum_{i = 2}^{D} z_i^2} + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{Q}\\Lambda^{\\!10}\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\nProperties:\nAs for the Bent Cigar function, a ridge defined as \\sum_{i=2}^D z_i^2 = 0 must be followed. The ridge is non-differentiable and the gradient is constant when the ridge is approached from any given point. Following the gradient becomes ineffective close to the ridge where the ridge needs to be followed in z_1-direction to its optimum. The necessary change in “search behavior” close to the ridge is difficult to diagnose, because the gradient towards the ridge does not flatten out.\nInformation gained from this function:\n\nIn comparison to f12: What is the effect of non-smoothness, non-differentiabale ridge?\n\n\n\n\n\nf14: Different Powers Function\nf_{14}(\\mathbf{x}) = \\sqrt{\\sum_{i = 1}^{D}|z_i|^{2+4\\frac{i - 1}{D- 1}}} + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\nProperties:\nDue to the different exponents the sensitivies of the z_i-variables become more and more different when approaching the optimum.\n\n\n\n\nf15: Rastrigin Function\nf_{15}(\\mathbf{x}) = 10 \\left(D- \\sum_{i = 1}^{D}\\cos(2\\pi z_i)\\right)\n    + \\|\\mathbf{z}\\|^2 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{R}\\Lambda^{\\!10}\\mathbf{Q}\\,T^{{0.2}}_\\mathrm{asy}(T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}})))\n\nProperties:\nPrototypical highly multimodal function which has originally a very regular and symmetric structure for the placement of the optima. The transformations T^{{}}_\\mathrm{asy} and T_\\mathrm{\\hspace*{-0.01em}osz} alleviate the symmetry and regularity of the original Rastrigin function.\n\nnon-separable less regular counterpart of f_3\nroughly 10^D local optima\nconditioning is about 10\n\nInformation gained from this function:\n\nin comparison to f3: What is the effect of non-separability for a highly multimodal function?\n\n\n\n\n\nf16: Weierstrass Function\nf_{16}(\\mathbf{x}) = 10 \\left( \\frac{1}{D}\n   \\sum_{i = 1}^{D}\\sum_{k = 0}^{11} 1/2^k \\cos(2\\pi3^k(z_i + 1/2))\n            - f_0 \\right)^3 + \\frac{10}{D}f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\mathbf{R}\\Lambda^{\\!1/100}\\mathbf{Q}\\,T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\nf_0 = \\sum_{k = 0}^{11} 1/2^k \\cos(2\\pi3^k 1/2)\n\nProperties:\nHighly rugged and moderately repetitive landscape, where the global optimum is not unique.\n\nthe term \\sum_k 1/2^k \\cos(2\\pi3^k\\dots) introduces the ruggedness, where lower frequencies have a larger weight 1/2^k.\nrotated, locally irregular, non-unique global optimum\n\nInformation gained from this function:\n\nin comparison to f17: Does ruggedness or a repetitive landscape deter the search behavior?\n\n\n\n\n\nf17: Schaffers F7 Function\nf_{17}(\\mathbf{x}) = \\left(\\frac{1}{D- 1}\\sum_{i = 1}^{D- 1} \\sqrt{s_i} +\n      \\sqrt{s_i} \\sin^2\\!\\left(50\\,s_i^{1/5}\\right)\\right)^2 + 10\\,f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\Lambda^{\\!10}\\mathbf{Q}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\ns_i = \\sqrt{z_i^2 + z_{i+1}^2} for i=1,\\dots,D\n\nProperties:\nA highly multimodal function where frequency and amplitude of the modulation vary.\n\nasymmetric, rotated\nconditioning is low\n\n\n\n\n\nf18: Schaffers F7 Function, moderately ill-conditioned\nf_{18}(\\mathbf{x}) = \\left(\\frac{1}{D- 1}\\sum_{i = 1}^{D- 1} \\sqrt{s_i} +\n      \\sqrt{s_i} \\sin^2\\!\\left(50\\,s_i^{1/5}\\right)\\right)^2 + 10\\,f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\\mathbf{z}= \\Lambda^{\\!1000}\\mathbf{Q}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\ns_i = \\sqrt{z_i^2 + z_{i+1}^2} for i=1,\\dots,D\n\nProperties:\nModerately ill-conditioned counterpart to f_{17}\n\nconditioning of about 1000\n\nInformation gained from this function:\n\nIn comparison to f17: What is the effect of ill-conditioning?\n\n\n\n\n\nf19: Composite Griewank-Rosenbrock Function F8F2\nf_{19}(\\mathbf{x}) = \\frac{10}{D-1} \\sum_{i=1}^{D-1} \\left( \\frac{s_i}{4000} - \\cos(s_i) \\right) + 10 + f_\\mathrm{opt}\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)\\mathbf{R}\\mathbf{x}+ 0.5\ns_i = 100\\,(z_i^2 - z_{i+1})^2 + (z_i-1)^2 for i=1,\\dots,D\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nResembling the Rosenbrock function in a highly multimodal way.\n\n\n\n\nf20: Schwefel Function\nf_{20}(\\mathbf{x}) = - \\frac{1}{100D}%  % kept in the final print\n\\sum_{i = 1}^{D}z_i\\sin(\\sqrt{|z_i|}) + 4.189828872724339\n     + 100f_{\\mathrm{pen}}(\\mathbf{z}/100) + f_\\mathrm{opt}\n\n\\hat{\\mathbf{x}} = 2\\times\\mathbf{1_-^+}\\otimes\\mathbf{x}\n\\hat{z}_{1} = \\hat{x}_{1},\\quad \\hat{z}_{i+1} = \\hat{x}_{i+1} + 0.25 \\left({\\hat{x}_{i}} - 2|x_i^{\\text{opt}}| \\right),\\quad \\text{for } i = 1, \\ldots, D - 1\n\\mathbf{z}= 100 (\\Lambda^{10} (\\hat{\\mathbf{z}} - 2\\left|\\mathbf{x^\\mathrm{opt}}\\right|) + 2\\left|\\mathbf{x^\\mathrm{opt}}\\right|)\n\\mathbf{x^\\mathrm{opt}}= 4.2096874633/2 \\;\\mathbf{1_-^+}, where \\mathbf{1}_-^+ is the same realization as above\n\nProperties:\nThe most prominent 2^D minima are located comparatively close to the corners of the unpenalized search area.\n\nthe penalization is essential, as otherwise more and better minima occur further away from the search space origin\n\n\n\n\n\nf21: Gallagher’s Gaussian 101-me Peaks Function\nf_{21}(\\mathbf{x}) = T_\\mathrm{\\hspace*{-0.01emosz}}\\left( 10 - \\max_{i=1}^{101}\n       w_i \\exp\\left(-\\frac{1}{2D}\\,\n           (\\mathbf{x}-\\mathbf{y}_i)^{\\mathrm{T}}\\mathbf{R}^{\\mathrm{T}}\n           \\mathbf{C}_i \\mathbf{R}(\\mathbf{x}-\\mathbf{y}_i) \\right) \\right)^2 + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\nw_i = \\begin{cases}  1.1 + 8 \\times\\dfrac{i-2}{99} & \\text{for~} i=2,\\dots,101 \\\\  10 & \\text{for~} i = 1  \\end{cases}, three optima have a value larger than 9\n\\mathbf{C}_i=\\Lambda^{\\!\\alpha_i}/\\alpha_i^{1/4} where \\Lambda^{\\!\\alpha_i} is defined as usual, but with randomly permuted diagonal elements. For i=2,\\dots,101, \\alpha_i is drawn uniformly randomly from the set \\left\\{1000^{2\\frac{j}{99}} ~|~ j =  0,\\dots,99\\right\\} without replacement, and \\alpha_i=1000 for i=1.\nthe local optima \\mathbf{y}_i are uniformly drawn from the domain [-5,5]^D for i=2,\\dots,101 and \\mathbf{y}_{1}\\in[-4,4]^D. The global optimum is at \\mathbf{x^\\mathrm{opt}}=\\mathbf{y}_1.\n\nProperties:\nThe function consists of 101 optima with position and height being unrelated and randomly chosen (different for each instantiation of the function).\n\nthe conditioning around the global optimum is about 30\n\nInformation gained from this function:\n\nIs the search effective without any global structure?\n\n\n\n\n\nf22: Gallagher’s Gaussian 21-hi Peaks Function\nf_{22}(\\mathbf{x}) = T_\\mathrm{\\hspace*{-0.01emosz}}\\left( 10 - \\max_{i=1}^{21}\n       w_i \\exp\\left(-\\frac{1}{2D}\\,\n           (\\mathbf{x}-\\mathbf{y}_i)^{\\mathrm{T}}\\mathbf{R}^{\\mathrm{T}}\n           \\mathbf{C}_i \\mathbf{R}(\\mathbf{x}-\\mathbf{y}_i) \\right) \\right)^2 + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\nw_i = \\begin{cases}  1.1 + 8 \\times\\dfrac{i-2}{19} & \\text{for~} i=2,\\dots,21 \\\\  10 & \\text{for~} i = 1  \\end{cases}, two optima have a value larger than 9\n\\mathbf{C}_i=\\Lambda^{\\!\\alpha_i}/\\alpha_i^{1/4} where \\Lambda^{\\!\\alpha_i} is defined as usual, but with randomly permuted diagonal elements. For i=2,\\dots,21, \\alpha_i is drawn uniformly randomly from the set \\left\\{1000^{2\\frac{j}{19}} ~|~ j =  0,\\dots,19\\right\\} without replacement, and \\alpha_i=1000^2 for i=1.\nthe local optima \\mathbf{y}_i are uniformly drawn from the domain [-4.9,4.9]^D for i=2,\\dots,21 and \\mathbf{y}_{1}\\in[[-4,4]^D \\longrightarrow]  [-3.92,3.92]^D]. The global optimum is at \\mathbf{x^\\mathrm{opt}}=\\mathbf{y}_1.\n\nProperties:\nThe function consists of 21 optima with position and height being unrelated and randomly chosen (different for each instantiation of the function).\n\nthe conditioning around the global optimum is about 1000\n\nInformation gained from this function:\n\nIn comparison to f21: What is the effect of higher condition?\n\n\n\n\n\nf23: Katsuura Function\nf_{23}(\\mathbf{x}) = \\frac{10}{D^{2}} \\prod_{i=1}^D\\left(1 + i\\sum_{j=1}^{32}\n     \\frac{\\left|2^j z_i - [2^j z_i]\\right|}{2^j} \\right)^{10/D^{1.2}}\n       - \\frac{10}{D^{2}} + f_{\\mathrm{pen}}(\\mathbf{x}) + f_{\\mathrm{opt}}(\\mathbf{x})\n\n\\mathbf{z}= \\mathbf{Q}\\,\\Lambda^{\\!100}\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}})\n\nProperties:\nHighly rugged and highly repetitive function with more than 10^D global optima.\n\n\n\n\nf24: Lunacek bi-Rastrigin Function\nf_{24}(\\mathbf{x}) = \\mathrm{min}\\left(\\sum_{i = 1}^{D}(\\hat{x}_i - \\mu_0)^2,\n                       d\\,D+ s\\sum_{i = 1}^{D}(\\hat{x}_i - \\mu_1)^2\\right)\n           + 10\\left(D- \\sum_{i=1}^D\\cos(2\\pi z_i)\\right) + 10^4\\,f^{{}_\\mathrm{pen}}(\\mathbf{x}) + f_{\\mathrm{opt}}(\\mathbf{x})\n\n\\hat{\\mathbf{x}} = 2\\,\\mathrm{{sign}}(\\mathbf{x^\\mathrm{opt}})\\otimes\\mathbf{x}, \\mathbf{x^\\mathrm{opt}}= [\\mu_0 \\longrightarrow]  \\frac{\\mu_0}{2} \\mathbf{1_-^+}\n\\mathbf{z}= \\mathbf{Q}\\Lambda^{\\!100}\\mathbf{R}(\\hat{\\mathbf{x}}-\\mu_0\\,\\mathbf{1})\n\\mu_0 = 2.5, \\mu_1 = -\\sqrt{\\dfrac{\\mu_0^2-d}{s}}, s = 1 - \\dfrac{1}{2\\sqrt{D+20}-8.2}, d=1\n\nProperties:\nHighly multimodal function with two funnels around [\\mu_0\\mathbf{1_-^+} \\longrightarrow]  \\frac{\\mu_0}{2}\\mathbf{1_-^+} and [-\\mu_1\\mathbf{1_-^+} \\longrightarrow]  \\frac{\\mu_1}{2}\\mathbf{1_-^+} being superimposed by the cosine. Presumably different approaches need to be used for “selecting the funnel” and for search the highly multimodal function “within” the funnel. The function was constructed to be deceptive for evolutionary algorithms with large population size.\n\nthe funnel of the local optimum at [-\\mu_1 \\mathbf{1_-^+}]  \\frac{\\mu_1}{2}\\mathbf{1_-^+} has roughly 70\\% of the search space volume within [-5,5]^D.\n\nInformation gained from this function: Can the search behavior be local on the global scale but global on a local scale?"
  },
  {
    "objectID": "mixed-integer/def.html",
    "href": "mixed-integer/def.html",
    "title": "Function definitions",
    "section": "",
    "text": "The bbob-mixint suite is constructed by partially discretizing problems from the bbob and bbob-largescale suites. In the following, we first explain how the discretization is performed, then describe the construction of the suite and finally show how the functions are scaled to adjust their difficulty.\n\n\nConsider a bbob(or bbob-largescale) problem with the function f: \\mathbb{R}^n \\to \\mathbb{R} and optimal value f^{\\textrm{opt}} = f(\\mathbf{x}^{\\textrm{opt}}). The resulting mixed-integer function will have the form \\overline{f}: \\mathbb{Z}^k \\times \\mathbb{R}^{(n-k)} \\to \\mathbb{R}, that is, it will be defined on k integer and n-k continuous variables. While all bbob functions are defined for any \\mathbf{x} \\in \\mathbb{R}^n, all but the linear slope function have their optimal solution within [-4, 4]^n. The partial discretization is performed in such a way that the optimal value is preserved, that is \\vphantom{f}\\smash{\\overline f}^{\\textrm{opt}} = f^{\\textrm{opt}}.\nNow let us assume that we wish to discretize the variable x_i, where i \\in \\{1, \\dots, k\\}, into the set \\{0, \\dots, l-1\\} of l integer values. This discretization is done as follows:\n\n First, we define a sequence of l equidistant auxiliary values -4 &lt; y_1 &lt; \\cdots &lt; y_l &lt; 4 so that y_{j+1}-y_j=\\frac{4+4}{l + 1}=y_1-(-4)=4-y_l, where j = 1, \\dots, l - 1.\nWe denote with y^{*} the value y_j, j = 1, \\dots, l, that is closest to x_i^{\\text{opt}}. The difference between the two is represented by d_i = y^{*} - x_i^{\\text{opt}}. Note that |d_i| \\leq \\frac{4}{l+1} if x_i^{\\text{opt}} \\in [-4, 4].\nThen, z_j = y_j - d_i for j = 1, \\dots, l. This aligns one of the z_j values with x_i^{\\text{opt}}.\n Finally, the following transformation \\zeta is used to map any continuous value x_i \\in \\mathbb{R} to an integer in \\{0, \\dots, l-1\\}: \\zeta (x_i) =\n          \\left\\{\\begin{array}{cl}\n            0 &\\quad\\text{if}\\quad x_i &lt; z_1 + \\frac{4}{l+1} \\\\[0.2em]\n            1 &\\quad\\text{if}\\quad z_1 + \\frac{4}{l+1} \\leq x_i &lt; z_2 + \\frac{4}{l+1} \\\\\n            \\vdots & \\quad\\vdots \\\\\n            l-1 &\\quad\\text{if}\\quad z_{l-1} + \\frac{4}{l+1} \\leq x_i\n           \\end{array}\\right.\n\nThe values y_j, j = 1, \\dots, l, in Step (1) are chosen in such a way that the corresponding shifted values z_j remain within [-4, 4] if x_i^{\\text{opt}} is also in [-4, 4]. If not, the shift is larger, but z_j, j = 1, \\dots, l, never go beyond x_i^{\\text{opt}}, which in practice means they remain within [-5, 5]^n—the region of interest for all bbob problems.\n\n\n\nThe bbob suite consists of problems with 24 different functions in 6 dimensions, n = 2, 3, 5, 10, 20, 40, and 15 instances. Because the discretization reduces the number of continuous variables, higher dimensions are used for the bbob-mixint suite to produce challenging problems. We chose n = 5, 10, 20, 40, 80, 160 as the dimensions of the bbob-mixint suite.\nBecause the numerical effort for some bbob problems scales with n^2, we use these for dimensions \\leq 40 only. For dimensions &gt;40, we use the corresponding problems from the bbob-largescale suite which scale linearly with n.\nAs all dimensions n are multiples of 5, we define five arities for n/5 consecutive variables, respectively, as l=2,4,8,16,\\infty. We use instances 1–15 to instantiate each problem. They match the equally-numbered instances of the underlying bbob and bbob-largescale problems.\n\n\n\nInitial experiments using the algorithms Random Search, CMA-ES and DE have shown that the new problems are of considerably different difficulties. Some are extremely hard to solve, while for others, a non-negligible percentage of targets is met already after a handful of function evaluations. Because COCO’s performance assessment aggregates results over function and target pairs, we scale function values to adjust for these different difficulties.\nIn order to decide on the scaling factors, we look at how many targets can be reached just by evaluating the domain middle (often the first guess of an optimization algorithm). However, because two values could be interpreted as the ‘middle’ value for variables of even arity, we need to sample among a large set of possible domain middle points. Figure 1 (b) shows the difference between the median f-value of 1000 domain middle samples and the f-value of the optimal solution for each problem instance in the bbob-mixint suite prior to scaling. In comparison, Figure 1 (a) shows the difference between the f-value of the domain middle and of the optimal solution for each problem instance for the bbob suite (note that no sampling is required here since it is clear which point is the domain middle in a continuous domain).\nKeeping in mind that in COCO the easiest target defaults to 100, we see that for a number of bbob-mixint functions (f_2, f_6, f_{10} to f_{13} and f_{20}), the domain middle rarely (if ever) achieves this target. On the other hand, for functions such as f_{17}, f_{19} and f_{23}, evaluating the domain middle already guarantees reaching targets of 10 and less. We also see that the distances for the bbob-mixint suite are very similar to those for the bbob suite, albeit a bit larger in general. Based on these findings and the preliminary algorithm results, we choose to multiply the f-values of the functions with the scaling factors shown in Table. This setting is mindful of the connections between some functions, for example, the same scaling factors are used for the original (f_8) and rotated (f_9) Rosenbrock functions. Figure 1 (c) shows the result for all (scaled) bbob-mixint problems. Now the f-differences between the domain middle and the optimal solution are more uniform across the problems in the suite.\n\n\n\n\n\n\n\n(a) bbob suite\n\n\n\n\n\n\n\n(b) bbob-mixint suite before scaling\n\n\n\n\n\n\n\n(c) bbob-mixint suite\n\n\n\n\nFigure 1: Estimation of targets reached by the domain middle (in logarithmic scale). They are computed as the distance between the f -value of the domain middle and the optimal solution for the bbob suite (a), or the median of the distances between the f -values of 1000 domain middle samples and the optimal solution for the bbob-mixint suite before (b) and after (c) scaling. Each circle depicts one problem instance for instances 1–15.\n\n\n\nTable 1: Factors used for scaling the bbob-mixint functions.\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nValue\nFactor\nValue\nFactor\nValue\nFactor\nValue\n\n\n\n\nf_1\n1\nf_7\n1\nf_{13}\n0.1\nf_{19}\n10\n\n\nf_2\n10^{-3}\nf_8\n10^{-2}\nf_{14}\n1\nf_{20}\n0.1\n\n\nf_3\n0.1\nf_9\n10^{-2}\nf_{15}\n0.1\nf_{21}\n1\n\n\nf_4\n0.1\nf_{10}\n10^{-3}\nf_{16}\n1\nf_{22}\n1\n\n\nf_5\n1\nf_{11}\n10^{-2}\nf_{17}\n10\nf_{23}\n10\n\n\nf_6\n10^{-2}\nf_{12}\n10^{-4}\nf_{18}\n1\nf_{24}\n0.1\n\n\n\nTo summarize, the bbob-mixint suite contains mixed-integer problems constructed by discretizing the continuous problems from the bbob and bbob-largescale suites. Using 24 functions, 6 dimensions and 15 instances results in the total of 2160 problem instances."
  },
  {
    "objectID": "mixed-integer/def.html#the-bbob-mixint-suite",
    "href": "mixed-integer/def.html#the-bbob-mixint-suite",
    "title": "Function definitions",
    "section": "",
    "text": "The bbob-mixint suite is constructed by partially discretizing problems from the bbob and bbob-largescale suites. In the following, we first explain how the discretization is performed, then describe the construction of the suite and finally show how the functions are scaled to adjust their difficulty.\n\n\nConsider a bbob(or bbob-largescale) problem with the function f: \\mathbb{R}^n \\to \\mathbb{R} and optimal value f^{\\textrm{opt}} = f(\\mathbf{x}^{\\textrm{opt}}). The resulting mixed-integer function will have the form \\overline{f}: \\mathbb{Z}^k \\times \\mathbb{R}^{(n-k)} \\to \\mathbb{R}, that is, it will be defined on k integer and n-k continuous variables. While all bbob functions are defined for any \\mathbf{x} \\in \\mathbb{R}^n, all but the linear slope function have their optimal solution within [-4, 4]^n. The partial discretization is performed in such a way that the optimal value is preserved, that is \\vphantom{f}\\smash{\\overline f}^{\\textrm{opt}} = f^{\\textrm{opt}}.\nNow let us assume that we wish to discretize the variable x_i, where i \\in \\{1, \\dots, k\\}, into the set \\{0, \\dots, l-1\\} of l integer values. This discretization is done as follows:\n\n First, we define a sequence of l equidistant auxiliary values -4 &lt; y_1 &lt; \\cdots &lt; y_l &lt; 4 so that y_{j+1}-y_j=\\frac{4+4}{l + 1}=y_1-(-4)=4-y_l, where j = 1, \\dots, l - 1.\nWe denote with y^{*} the value y_j, j = 1, \\dots, l, that is closest to x_i^{\\text{opt}}. The difference between the two is represented by d_i = y^{*} - x_i^{\\text{opt}}. Note that |d_i| \\leq \\frac{4}{l+1} if x_i^{\\text{opt}} \\in [-4, 4].\nThen, z_j = y_j - d_i for j = 1, \\dots, l. This aligns one of the z_j values with x_i^{\\text{opt}}.\n Finally, the following transformation \\zeta is used to map any continuous value x_i \\in \\mathbb{R} to an integer in \\{0, \\dots, l-1\\}: \\zeta (x_i) =\n          \\left\\{\\begin{array}{cl}\n            0 &\\quad\\text{if}\\quad x_i &lt; z_1 + \\frac{4}{l+1} \\\\[0.2em]\n            1 &\\quad\\text{if}\\quad z_1 + \\frac{4}{l+1} \\leq x_i &lt; z_2 + \\frac{4}{l+1} \\\\\n            \\vdots & \\quad\\vdots \\\\\n            l-1 &\\quad\\text{if}\\quad z_{l-1} + \\frac{4}{l+1} \\leq x_i\n           \\end{array}\\right.\n\nThe values y_j, j = 1, \\dots, l, in Step (1) are chosen in such a way that the corresponding shifted values z_j remain within [-4, 4] if x_i^{\\text{opt}} is also in [-4, 4]. If not, the shift is larger, but z_j, j = 1, \\dots, l, never go beyond x_i^{\\text{opt}}, which in practice means they remain within [-5, 5]^n—the region of interest for all bbob problems.\n\n\n\nThe bbob suite consists of problems with 24 different functions in 6 dimensions, n = 2, 3, 5, 10, 20, 40, and 15 instances. Because the discretization reduces the number of continuous variables, higher dimensions are used for the bbob-mixint suite to produce challenging problems. We chose n = 5, 10, 20, 40, 80, 160 as the dimensions of the bbob-mixint suite.\nBecause the numerical effort for some bbob problems scales with n^2, we use these for dimensions \\leq 40 only. For dimensions &gt;40, we use the corresponding problems from the bbob-largescale suite which scale linearly with n.\nAs all dimensions n are multiples of 5, we define five arities for n/5 consecutive variables, respectively, as l=2,4,8,16,\\infty. We use instances 1–15 to instantiate each problem. They match the equally-numbered instances of the underlying bbob and bbob-largescale problems.\n\n\n\nInitial experiments using the algorithms Random Search, CMA-ES and DE have shown that the new problems are of considerably different difficulties. Some are extremely hard to solve, while for others, a non-negligible percentage of targets is met already after a handful of function evaluations. Because COCO’s performance assessment aggregates results over function and target pairs, we scale function values to adjust for these different difficulties.\nIn order to decide on the scaling factors, we look at how many targets can be reached just by evaluating the domain middle (often the first guess of an optimization algorithm). However, because two values could be interpreted as the ‘middle’ value for variables of even arity, we need to sample among a large set of possible domain middle points. Figure 1 (b) shows the difference between the median f-value of 1000 domain middle samples and the f-value of the optimal solution for each problem instance in the bbob-mixint suite prior to scaling. In comparison, Figure 1 (a) shows the difference between the f-value of the domain middle and of the optimal solution for each problem instance for the bbob suite (note that no sampling is required here since it is clear which point is the domain middle in a continuous domain).\nKeeping in mind that in COCO the easiest target defaults to 100, we see that for a number of bbob-mixint functions (f_2, f_6, f_{10} to f_{13} and f_{20}), the domain middle rarely (if ever) achieves this target. On the other hand, for functions such as f_{17}, f_{19} and f_{23}, evaluating the domain middle already guarantees reaching targets of 10 and less. We also see that the distances for the bbob-mixint suite are very similar to those for the bbob suite, albeit a bit larger in general. Based on these findings and the preliminary algorithm results, we choose to multiply the f-values of the functions with the scaling factors shown in Table. This setting is mindful of the connections between some functions, for example, the same scaling factors are used for the original (f_8) and rotated (f_9) Rosenbrock functions. Figure 1 (c) shows the result for all (scaled) bbob-mixint problems. Now the f-differences between the domain middle and the optimal solution are more uniform across the problems in the suite.\n\n\n\n\n\n\n\n(a) bbob suite\n\n\n\n\n\n\n\n(b) bbob-mixint suite before scaling\n\n\n\n\n\n\n\n(c) bbob-mixint suite\n\n\n\n\nFigure 1: Estimation of targets reached by the domain middle (in logarithmic scale). They are computed as the distance between the f -value of the domain middle and the optimal solution for the bbob suite (a), or the median of the distances between the f -values of 1000 domain middle samples and the optimal solution for the bbob-mixint suite before (b) and after (c) scaling. Each circle depicts one problem instance for instances 1–15.\n\n\n\nTable 1: Factors used for scaling the bbob-mixint functions.\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nValue\nFactor\nValue\nFactor\nValue\nFactor\nValue\n\n\n\n\nf_1\n1\nf_7\n1\nf_{13}\n0.1\nf_{19}\n10\n\n\nf_2\n10^{-3}\nf_8\n10^{-2}\nf_{14}\n1\nf_{20}\n0.1\n\n\nf_3\n0.1\nf_9\n10^{-2}\nf_{15}\n0.1\nf_{21}\n1\n\n\nf_4\n0.1\nf_{10}\n10^{-3}\nf_{16}\n1\nf_{22}\n1\n\n\nf_5\n1\nf_{11}\n10^{-2}\nf_{17}\n10\nf_{23}\n10\n\n\nf_6\n10^{-2}\nf_{12}\n10^{-4}\nf_{18}\n1\nf_{24}\n0.1\n\n\n\nTo summarize, the bbob-mixint suite contains mixed-integer problems constructed by discretizing the continuous problems from the bbob and bbob-largescale suites. Using 24 functions, 6 dimensions and 15 instances results in the total of 2160 problem instances."
  },
  {
    "objectID": "noisy/def.html",
    "href": "noisy/def.html",
    "title": "Function definitions",
    "section": "",
    "text": "Used symbols and definitions of, e.g., auxiliary functions are given in the following. Vectors are typeset in bold and refer to column vectors.\n\\otimes : indicates element-wise multiplication of two D-dimensional vectors, \\otimes:\\mathcal{R}^D\\times\\mathcal{R}^D\\to\\mathcal{R}^D,  (\\mathbf{x},\\mathbf{y})\\mapsto\\mathrm{{diag}}(\\mathbf{x})\\times\\mathbf{y}=(x_i\\times y_i)_{i=1,\\dots,D}\n\\|.\\| : denotes the Euclidean norm, \\|\\mathbf{x}\\|^2=\\sum_i x_i^2.\n[.] : denotes the nearest integer value\n\\mathbf{0} : =(0,\\dots,0)^{\\mathrm{T}} all zero vector\n\\mathbf{1} : =(1,\\dots,1)^{\\mathrm{T}} all one vector\n\\Lambda^{\\!\\alpha} : is a diagonal matrix in D dimensions with the ith diagonal element as \\lambda_{ii} = \\alpha^{\\frac{1}{2}\\frac{i-1}{D-1}}\nf^{{}}_\\mathrm{pen} : :\\mathcal{R}^D\\to\\mathcal{R}, \\mathbf{x}\\mapsto 100\\sum_{i=1}^D\\max(0,|x_i| - 5)^2\n\\mathbf{1}_-^+ : a D-dimensional vector with entries of -1 or 1 both drawn equal probability.\n\\mathbf{Q}, \\mathbf{R} : orthogonal (rotation) matrices. For each function instance in each dimension a single realization for respectively \\mathbf{Q} and \\mathbf{R} is used. Rotation matrices are generated from standard normally distributed entries by Gram-Schmidt orthogonalization. Columns and rows of a rotation matrix form an orthogonal basis.\n\\mathbf{R} : see \\mathbf{Q}\nT^{{\\beta}_\\mathrm{asy}} : :\\mathcal{R}^D\\to\\mathcal{R}^D, x_i\\mapsto  \\begin{cases}  x_i^{1+\\beta \\frac{i-1}{D-1}\\sqrt{x_i}} & \\text{~if~} x_i&gt;0\\\\  x_i & \\text{~otherwise~}  \\end{cases}, for i=1,\\dots,D.\nT_\\mathrm{\\hspace*{-0.01em}osz} : :\\mathcal{R}^n\\to\\mathcal{R}^n, for any positive integer n, maps element-wise x\\mapsto\\mathrm{{sign}}(x)\\exp\\left(\\hat{x} +\n       0.049\\left(\\sin(c_1 \\hat{x}) + \\sin(c_2 \\hat{x})\\right)\\right) with \\hat{x}= \\begin{cases}  \\log(|x|) & \\text{if~} x\\not=0 \\\\  \\in\\mathcal{R}& \\text{otherwise}  \\end{cases}, \\mathrm{{sign}}(x) = \\begin{cases} -1 & \\text{if~} x &lt; 0 \\\\  0 & \\text{if~} x = 0 \\\\  1 & \\text{otherwise}  \\end{cases}, c_1 = \\begin{cases}  10 & \\text{if~} x &gt; 0\\\\  5.5 & \\text{otherwise}  \\end{cases} and c_2 = \\begin{cases}  7.9 & \\text{if~} x &gt; 0\\\\  3.1 & \\text{otherwise}  \\end{cases}\n\\mathbf{x}^\\mathrm{opt} : optimal solution vector, such that f(\\mathbf{x^\\mathrm{opt}}) is minimal."
  },
  {
    "objectID": "noisy/def.html#symbols-and-definitions",
    "href": "noisy/def.html#symbols-and-definitions",
    "title": "Function definitions",
    "section": "",
    "text": "Used symbols and definitions of, e.g., auxiliary functions are given in the following. Vectors are typeset in bold and refer to column vectors.\n\\otimes : indicates element-wise multiplication of two D-dimensional vectors, \\otimes:\\mathcal{R}^D\\times\\mathcal{R}^D\\to\\mathcal{R}^D,  (\\mathbf{x},\\mathbf{y})\\mapsto\\mathrm{{diag}}(\\mathbf{x})\\times\\mathbf{y}=(x_i\\times y_i)_{i=1,\\dots,D}\n\\|.\\| : denotes the Euclidean norm, \\|\\mathbf{x}\\|^2=\\sum_i x_i^2.\n[.] : denotes the nearest integer value\n\\mathbf{0} : =(0,\\dots,0)^{\\mathrm{T}} all zero vector\n\\mathbf{1} : =(1,\\dots,1)^{\\mathrm{T}} all one vector\n\\Lambda^{\\!\\alpha} : is a diagonal matrix in D dimensions with the ith diagonal element as \\lambda_{ii} = \\alpha^{\\frac{1}{2}\\frac{i-1}{D-1}}\nf^{{}}_\\mathrm{pen} : :\\mathcal{R}^D\\to\\mathcal{R}, \\mathbf{x}\\mapsto 100\\sum_{i=1}^D\\max(0,|x_i| - 5)^2\n\\mathbf{1}_-^+ : a D-dimensional vector with entries of -1 or 1 both drawn equal probability.\n\\mathbf{Q}, \\mathbf{R} : orthogonal (rotation) matrices. For each function instance in each dimension a single realization for respectively \\mathbf{Q} and \\mathbf{R} is used. Rotation matrices are generated from standard normally distributed entries by Gram-Schmidt orthogonalization. Columns and rows of a rotation matrix form an orthogonal basis.\n\\mathbf{R} : see \\mathbf{Q}\nT^{{\\beta}_\\mathrm{asy}} : :\\mathcal{R}^D\\to\\mathcal{R}^D, x_i\\mapsto  \\begin{cases}  x_i^{1+\\beta \\frac{i-1}{D-1}\\sqrt{x_i}} & \\text{~if~} x_i&gt;0\\\\  x_i & \\text{~otherwise~}  \\end{cases}, for i=1,\\dots,D.\nT_\\mathrm{\\hspace*{-0.01em}osz} : :\\mathcal{R}^n\\to\\mathcal{R}^n, for any positive integer n, maps element-wise x\\mapsto\\mathrm{{sign}}(x)\\exp\\left(\\hat{x} +\n       0.049\\left(\\sin(c_1 \\hat{x}) + \\sin(c_2 \\hat{x})\\right)\\right) with \\hat{x}= \\begin{cases}  \\log(|x|) & \\text{if~} x\\not=0 \\\\  \\in\\mathcal{R}& \\text{otherwise}  \\end{cases}, \\mathrm{{sign}}(x) = \\begin{cases} -1 & \\text{if~} x &lt; 0 \\\\  0 & \\text{if~} x = 0 \\\\  1 & \\text{otherwise}  \\end{cases}, c_1 = \\begin{cases}  10 & \\text{if~} x &gt; 0\\\\  5.5 & \\text{otherwise}  \\end{cases} and c_2 = \\begin{cases}  7.9 & \\text{if~} x &gt; 0\\\\  3.1 & \\text{otherwise}  \\end{cases}\n\\mathbf{x}^\\mathrm{opt} : optimal solution vector, such that f(\\mathbf{x^\\mathrm{opt}}) is minimal."
  },
  {
    "objectID": "noisy/def.html#general-setup",
    "href": "noisy/def.html#general-setup",
    "title": "Function definitions",
    "section": "General Setup",
    "text": "General Setup\n\nSearch Space\nAll functions are defined and can be evaluated over \\mathcal{R}^{D}, while the actual search domain is given as [-5,5]^{D}.\n\n\nLocation of the optimal \\mathbf{x}^\\mathrm{opt} and of f_\\mathrm{opt}=f(\\mathbf{x^\\mathrm{opt}})\nAll functions have their global optimum in [-5,5]^{D}. The majority of functions has the global optimum in [-4,4]^{D}. The value for f_\\mathrm{opt} is drawn from a cauchy distributed random variable, with roughly 50% of the values between -100 and 100. The value is rounded after two decimal places and the maximum and minimum are set to 1000 and -1000 respectively. In the function definitions a transformed variable vector \\mathbf{z} is often used instead of the argument \\mathbf{x}. The vector \\mathbf{z} has its optimum in \\mathbf{z^\\mathrm{opt}}=\\mathbf{0}, if not stated otherwise.\n\n\nBoundary Handling\nOn all functions a penalty boundary handling is applied as given with f^{{}}_\\mathrm{pen}.\n\n\nLinear Transformations\nLinear transformations of the search space are applied to derive non-separable functions from separable ones and to control the conditioning of the function.\n\n\nNon-Linear Transformations and Symmetry Breaking\nIn order to make relatively simple, but well understood functions less regular, on some functions non-linear transformations are applied in x- or f-space. Both transformations T_\\mathrm{\\hspace*{-0.01emosz}}:\\mathcal{R}^n\\to\\mathcal{R}^n, n\\in\\{1,D\\}, and T^{{}_\\mathrm{asy}}:\\mathcal{R}^D\\to\\mathcal{R}^D are defined coordinate-wise. They are smooth and have, coordinate-wise, a strictly positive derivative. T_\\mathrm{\\hspace*{-0.01emosz}} is oscillating about the identity, where the oscillation is scale invariant w.r.t. the origin. T^{{}}_\\mathrm{asy} is the identity for negative values. When T^{{}}_\\mathrm{asy} is applied, a portion of 1/2^D of the search space remains untransformed."
  },
  {
    "objectID": "noisy/def.html#noise-models",
    "href": "noisy/def.html#noise-models",
    "title": "Function definitions",
    "section": "Noise Models",
    "text": "Noise Models\nIn this benchmarking suite three different noise models are used. The first two, f_{\\mathrm{GN}}\\left({}\\right) and f_{\\mathrm{UN}}\\left({}\\right), are multiplicative noise models while the third model, f_{\\mathrm{CN}}\\left({}\\right), is an additive noise model. All noise models are applied to a function value f under the assumption that f\\ge0. All noise models reveal stochastic dominance between any two solutions and are therefore utility-free.\n\nGaussian Noise\nThe Gaussian noise model is scale invariant and defined as f_{\\mathrm{GN}\\left({f,\\beta}\\right)} = f\\times\\exp(\\beta\\,\\mathcal{N}(0,1))\\enspace. The noise strength is controlled with \\beta. The distribution of the noise is log-normal, thus no negative noise values can be sampled. For the benchmark functions with moderate noise \\beta = 0.01, otherwise \\beta = 1. For small values of \\beta this noise model resembles f\\times(1+\\beta\\,\\mathcal{N}(0,1)).\n\n\nUniform Noise\nThe uniform noise model is introduced as a more severe noise model than the Gaussian and is defined as \n  f_{\\mathrm{UN}\\left({f,\\alpha,\\beta}\\right)} = f\\times\\mathcal{U}(0,1)^{\\beta}\\max\\left(1, \\left(\\dfrac{10^9}{f + \\epsilon}\\right)^{\\alpha\\,\\mathcal{U}(0,1)}\\right)\\enspace. The noise model uses two random factors. The first factor is in the interval [0,1], uniformly distributed for \\beta=1. The second factor, \\max(\\dots), is \\ge1. The parameters \\alpha and \\beta control the noise strength. For moderate noise \\alpha = 0.01\\left(0.49 + {1}/{D}\\right) and \\beta = 0.01, otherwise \\alpha = 0.49 + {1}/{D} and \\beta = 1. Furthermore, \\epsilon is set to 10^{-99} in order to prevent division by zero.\nThe uniform noise model is not scale invariant. Due to the last factor the noise strength increases with decreasing (positive) value of f. Therefore the noise strength becomes more severe when approaching the optimum.\n\n\nCauchy Noise\nThe Cauchy noise model represents a different type of noise with two important aspects. First, only a comparatively small percentage of function values is disturbed by noise. Second, the noise distribution is comparatively “weird”. Large outliers occur once in a while, and because they stem from a continuous distribution they cannot be easily diagnosed. The Cauchy noise model is defined as f_{\\mathrm{CN}\\left({f,\\alpha,p}\\right)} = f + \\alpha\\,\\max\\left(0, 1000 + \\mathbb{I}_{\\left\\{\\mathcal{U}(0,1) &lt; p\\right\\}}\\dfrac{\\mathcal{N}(0,1)}{|\\mathcal{N}(0,1)|+\\epsilon}\\right)\\enspace, where \\alpha defines the noise strength and p determines the frequency of the noise disturbance. In the moderate noise case \\alpha = 0.01 and p = 0.05, otherwise \\alpha = 1 and p = 0.2. The summand of 1000 was chosen to sample positive and negative “outliers” (as the function value is cut from below, see next paragraph) and \\epsilon is set to 10^{-199}.\n\n\nFinal Function Value\nIn order to achieve a convenient testing for the target function value, in all noise models 1.01\\times10^{-8} is added to the function value and, if the input argument f is smaller than 10^{-8}, the undisturbed f is returned.\nf_\\mathrm{XX}(f,\\dots) \\gets \\begin{cases}f_\\mathrm{XX}(f,\\dots) + 1.01\\times10^{-8}& \\text{if~} f\\ge10^{-8}\\\\\n                                 f & \\text{otherwise}\n\\end{cases}"
  },
  {
    "objectID": "noisy/def.html#functions-with-moderate-noise",
    "href": "noisy/def.html#functions-with-moderate-noise",
    "title": "Function definitions",
    "section": "Functions with moderate noise",
    "text": "Functions with moderate noise\n\nSphere\nf_\\mathrm{sphere}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant\n\n\nf101: Sphere with moderate gaussian noise\nf_{101}(\\mathbf{x}) = f_{\\mathrm{GN}}({f_\\mathrm{sphere}(\\mathbf{x}),0.01}) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf102: Sphere with moderate uniform noise\nf_{102}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),0.01\\left(0.49 + \\dfrac{1}{D}\\right),0.01}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf103: Sphere with moderate seldom cauchy noise\nf_{103}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),0.01,0.05}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nRosenbrock\nf_\\mathrm{rosenbrock}(\\mathbf{x}) = \\sum_{i = 1}^{D-1} 100\\,\\left(z_i^2 - z_{i+1}\\right)^2 + (z_i-1)^2\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}) + 1\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nSo-called banana function due to its 2-D contour lines as a bent ridge (or valley). In the beginning, the prominent first term of the function definition attracts to the point \\mathbf{z}=\\mathbf{0}. Then, a long bending valley needs to be followed to reach the global optimum. The ridge changes its orientation D-1 times.\n\nin larger dimensions the function has a local optimum with an attraction volume of about 25%\n\n\nf104: Rosenbrock with moderate gaussian noise\nf_{104}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),0.01}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf105: Rosenbrock with moderate uniform noise\nf_{105}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),0.01\\left(0.49 + \\dfrac{1}{D}\\right),0.01}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf106: Rosenbrock with moderate seldom cauchy noise\nf_{106}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),0.01,0.05}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}"
  },
  {
    "objectID": "noisy/def.html#functions-with-severe-noise",
    "href": "noisy/def.html#functions-with-severe-noise",
    "title": "Function definitions",
    "section": "Functions with severe noise",
    "text": "Functions with severe noise\n\nSphere\nf_\\mathrm{sphere}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant\n\n\nf107: Sphere with gaussian noise\nf_{107}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf108: Sphere with uniform noise\nf_{108}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf109: Sphere with seldom cauchy noise\nf_{109}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{sphere}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nRosenbrock\nf_\\mathrm{rosenbrock}(\\mathbf{x}) = \\sum_{i = 1}^{D-1} 100\\,\\left(z_i^2 - z_{i+1}\\right)^2 + (z_i-1)^2\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}) + 1\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nSo-called banana function due to its 2-D contour lines as a bent ridge (or valley). In the beginning, the prominent first term of the function definition attracts to the point \\mathbf{z}=\\mathbf{0}. Then, a long bending valley needs to be followed to reach the global optimum. The ridge changes its orientation D-1 times.\n\na local optimum with an attraction volume of about 25%\n\n\nf110: Rosenbrock with gaussian noise\nf_{110}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf111: Rosenbrock with uniform noise\nf_{111}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf112: Rosenbrock with seldom cauchy noise\nf_{112}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{rosenbrock}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nStep ellipsoid\nf_\\mathrm{step}(\\mathbf{x}) = 0.1 \\max\\left(|\\hat{z}_1|/10^4,\\, \\sum_{i = 1}^{D} 10^{2\\frac{i-1}{D-1}} z_i^2\\right)\n\n\\hat{\\mathbf{z}} = \\Lambda^{\\!10}\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\\tilde{z}_i = \\begin{cases}  \\lfloor0.5+\\hat{z}_i\\rfloor & \\text{if~} \\hat{z}_i &gt; 0.5 \\\\  {\\lfloor0.5+10\\,\\hat{z}_i\\rfloor}/{10} & \\text{otherwise}  \\end{cases} for i=1,\\dots,D,\ndenotes the rounding procedure in order to produce the plateaus.\n\\mathbf{z}= \\mathbf{Q}\\tilde{\\mathbf{z}}\n\nProperties:\nThe function consists of many plateaus of different sizes. Apart from a small area close to the global optimum, the gradient is zero almost everywhere.\n\ncondition number is about 100\n\n\nf113: Step ellipsoid with gaussian noise\nf_{113}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{step}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf114: Step ellipsoid with uniform noise\nf_{114}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{step}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf115: Step ellipsoid with seldom cauchy noise\nf_{115}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{step}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nEllipsoid\nf_\\mathrm{ellipsoid}(\\mathbf{x}) = \\sum_{i = 1}^{D} 10^{4\\frac{i-1}{D-1}}z_i^2\n\n\\mathbf{z}= T_\\mathrm{\\hspace*{-0.01emosz}}(\\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}}))\n\nProperties:\nGlobally quadratic ill-conditioned function with smooth local irregularities.\n\ncondition number is 10^4\n\n\nf116: Ellipsoid with gaussian noise\nf_{116}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{ellipsoid}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf117: Ellipsoid with uniform noise\nf_{117}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{ellipsoid}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf118: Ellipsoid with seldom cauchy noise\nf_{118}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{ellipsoid}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nDifferent Powers\nf_\\mathrm{diffpowers}(\\mathbf{x}) = \\sqrt{\\sum_{i = 1}^{D}|z_i|^{2+4\\frac{i - 1}{D- 1}}}\n\n\\mathbf{z}= \\mathbf{R}(\\mathbf{x}- \\mathbf{x^\\mathrm{opt}})\n\nProperties:\nDue to the different exponents the sensitivies of the z_i-variables become more and more different when approaching the optimum.\n\nf119: Different Powers with gaussian noise\nf_{119}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{diffpowers}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf120: Different Powers with uniform noise\nf_{120}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{diffpowers}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf121: Different Powers with seldom cauchy noise\nf_{121}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{diffpowers}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}"
  },
  {
    "objectID": "noisy/def.html#highly-multi-modal-functions-with-severe-noise",
    "href": "noisy/def.html#highly-multi-modal-functions-with-severe-noise",
    "title": "Function definitions",
    "section": "Highly multi-modal functions with severe noise",
    "text": "Highly multi-modal functions with severe noise\n\nSchaffer’s F7\nf_\\mathrm{schaffer}(\\mathbf{x}) = \\left(\\frac{1}{D- 1}\\sum_{i = 1}^{D- 1} \\sqrt{s_i} +\n      \\sqrt{s_i} \\sin^2\\!\\left(50\\,s_i^{1/5}\\right)\\right)^2\n\n\\mathbf{z}= \\Lambda^{\\!10}\\mathbf{Q}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\ns_i = \\sqrt{z_i^2 + z_{i+1}^2} for i=1,\\dots,D\n\nProperties:\nA highly multimodal function where frequency and amplitude of the modulation vary.\n\nconditioning is low\n\n\nf122: Schaffer’s F7 with gaussian noise\nf_{122}(\\mathbf{x}) = f_{\\mathrm{GN}\\left({f_\\mathrm{schaffer}(\\mathbf{x}),1}\\right)} + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf123: Schaffer’s F7 with uniform noise\nf_{123}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{schaffer}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf124: Schaffer’s F7 with seldom cauchy noise\nf_{124}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{schaffer}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nComposite Griewank-Rosenbrock\nf_\\mathrm{f8f2}(\\mathbf{x}) = \\frac{1}{D-1} \\sum_{i=1}^{D-1}\n     \\left(\\frac{s_i}{4000} - \\cos(s_i)\\right) + 1\n\n\\mathbf{z}= \\max\\!\\left(1,\\frac{\\sqrt{D}}{8}\\right)\\mathbf{R}\\mathbf{x}+ 0.5\ns_i = 100\\,(z_i^2 - z_{i+1})^2 + (z_i-1)^2 for i=1,\\dots,D\n\\mathbf{z^\\mathrm{opt}}=\\mathbf{1}\n\nProperties:\nResembling the Rosenbrock function in a highly multimodal way.\n\nf125: Composite Griewank-Rosenbrock with gaussian noise\nf_{125}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{f8f2}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf126: Composite Griewank-Rosenbrock with uniform noise\nf_{126}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{f8f2}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf127: Composite Griewank-Rosenbrock with seldom cauchy noise\nf_{127}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{f8f2}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\n\nGallagher’s Gaussian Peaks, globally rotated\nf_\\mathrm{gallagher}(\\mathbf{x}) = T_\\mathrm{\\hspace*{-0.01emosz}}\\left(10 - \\max_{i=1}^{101}\n                w_i \\exp\\left(-\\frac{1}{2D}\\,\n               (\\mathbf{x}-\\mathbf{y}_i)^{\\mathrm{T}}\\mathbf{R}^{\\mathrm{T}}\n                \\mathbf{C}_i \\mathbf{R}(\\mathbf{x}-\\mathbf{y}_i) \\right)\\right)^2\n\nw_i = \\begin{cases}  1.1 + 8 \\times\\dfrac{i-2}{99} & \\text{for~} i=2,\\dots,101 \\\\  10 & \\text{for~} i = 1  \\end{cases}, three optima have a value larger than 9\n\\mathbf{C}_i=\\Lambda^{\\!\\alpha_i}/\\alpha_i^{1/4} where \\Lambda^{\\!\\alpha_i} is defined as usual, but with randomly permuted diagonal elements. For i=2,\\dots,101, \\alpha_i is drawn uniformly randomly from the set \\left\\{1000^{2\\frac{j}{99}} ~|~ j =  0,\\dots,99\\right\\} without replacement, and \\alpha_i=1000 for i=1.\nthe local optima \\mathbf{y}_i are uniformly drawn from the domain [-4.9,4.9]^D for i=2,\\dots,101 and \\mathbf{y}_{1}\\in[-4,4]^D. The global optimum is at \\mathbf{x^\\mathrm{opt}}=\\mathbf{y}_1.\n\nProperties:\nThe function consists of 101 optima with position and height being unrelated and randomly chosen.\n\ncondition number around the global optimum is about 30\nsame overall rotation matrix\n\n\nf128: Gallagher’s Gaussian Peaks 101-me with gaussian noise\nf_{128}(\\mathbf{x}) = f_{\\mathrm{GN}}\\left({f_\\mathrm{gallagher}(\\mathbf{x}),1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf129: Gallagher’s Gaussian Peaks 101-me with uniform noise\nf_{129}(\\mathbf{x}) = f_{\\mathrm{UN}}\\left({f_\\mathrm{gallagher}(\\mathbf{x}),0.49 + \\dfrac{1}{D},1}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}\n\n\nf130: Gallagher’s Gaussian Peaks 101-me with seldom cauchy noise\nf_{130}(\\mathbf{x}) = f_{\\mathrm{CN}}\\left({f_\\mathrm{gallagher}(\\mathbf{x}),1,0.2}\\right) + f_{\\mathrm{pen}}(\\mathbf{x}) + f_\\mathrm{opt}"
  }
]