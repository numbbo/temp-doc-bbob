[
  {
    "objectID": "bbob/vis.html",
    "href": "bbob/vis.html",
    "title": "Visualizations of problem landscapes",
    "section": "",
    "text": "Show plots in −  + columns (click on Dimension/Function/Instance/Visualization type below to show all plots for the chosen category)\n\n\n\n\nDimension\n\n\nFunction\n\n\nInstance\n\n\nVisualization type\n\n\n\n\n\n−\n\n +\n\n\n\n−\n\n +\n\n\n\n−\n\n +"
  },
  {
    "objectID": "bbob/vis.html#plots",
    "href": "bbob/vis.html#plots",
    "title": "Visualizations of problem landscapes",
    "section": "",
    "text": "Show plots in −  + columns (click on Dimension/Function/Instance/Visualization type below to show all plots for the chosen category)\n\n\n\n\nDimension\n\n\nFunction\n\n\nInstance\n\n\nVisualization type\n\n\n\n\n\n−\n\n +\n\n\n\n−\n\n +\n\n\n\n−\n\n +"
  },
  {
    "objectID": "bbob/vis.html#plot-explanation",
    "href": "bbob/vis.html#plot-explanation",
    "title": "Visualizations of problem landscapes",
    "section": "Plot explanation",
    "text": "Plot explanation\n\n\nSearch space cuts\nThe plots with search space cuts show the function value f along various lines in the search space that go through the global optimum \\mathbf{x}_\\mathrm{opt}. The colored lines change the value of only one variable x_i at a time keeping the rest fixed to \\mathbf{x}_\\mathrm{opt}. The gray line represents the line that goes through \\mathbf{x}_\\mathrm{opt} in the direction of the all-ones vector (i.e., in the diagonal direction). To improve visibility, only five colored lines are shown in the larger dimensions (D \\geq 10), corresponding to x_1, x_2, x_{\\lfloor D/2 \\rfloor}, x_{D-1} and x_D, where D is the search space dimension. The plots are shown in three variants:\n\nlin-lin: both axes are linear,\nlin-log: the x-axis is linear, the y-axis shows the difference between f and the optimal value f_\\mathrm{opt} on a logarithmic scale,\nlog-log: both axes are logarithmic, the x-axis shows the absolute difference to \\mathbf{x}_\\mathrm{opt} (positive directions presented as x_i and negative as -x_i), the y-axis shows the difference between f and f_\\mathrm{opt}.\n\n\n\n\n\nFunction value heatmap\nThe function value heatmap shows the function values f on a 2-D view of the search space that contains the optimal solution and is approximated by a grid. In addition to color-coded function values, the plots include level sets in gray hues. For dimensions larger than 2, the heatmaps of pairs of variables are organized into a matrix. To improve visibility, only five variables are included in the matrix in the larger dimensions (D \\geq 10), corresponding to x_1, x_2, x_{\\lfloor D/2 \\rfloor}, x_{D-1} and x_D, where D is the search space dimension.\n\n\n\n\nNormalized rank heatmap\nThe normalized rank heatmap shows, instead of absolute function values f, their normalized rank with 0 corresponding to the best rank and 1 to the worst one on a 2-D view of the search space that contains the optimal solution and is approximated by a grid. In addition to color-coded ranks, the plots include level sets in gray hues. For dimensions larger than 2, the heatmaps of pairs of variables are organized into a matrix. To improve visibility, only five variables are included in the matrix in the larger dimensions (D \\geq 10), corresponding to x_1, x_2, x_{\\lfloor D/2 \\rfloor}, x_{D-1} and x_D, where D is the search space dimension.\n\n\n\n\nSurface plot\nThe surface plot shows the function values f on a 3-D view of the search space and is available only for 2-D problems. To improve visibility, the z-axis is inverted, so that the global optimum is at the top of the plot."
  },
  {
    "objectID": "bbob/vis.html#problem-definition",
    "href": "bbob/vis.html#problem-definition",
    "title": "Visualizations of problem landscapes",
    "section": "Problem definition",
    "text": "Problem definition"
  },
  {
    "objectID": "bbob/vis.html#separable-functions",
    "href": "bbob/vis.html#separable-functions",
    "title": "Visualizations of problem landscapes",
    "section": "Separable functions",
    "text": "Separable functions"
  },
  {
    "objectID": "bbob/vis.html#functions-with-low-or-moderate-conditioning",
    "href": "bbob/vis.html#functions-with-low-or-moderate-conditioning",
    "title": "Visualizations of problem landscapes",
    "section": "Functions with low or moderate conditioning",
    "text": "Functions with low or moderate conditioning"
  },
  {
    "objectID": "bbob/vis.html#functions-with-high-conditioning-and-unimodal",
    "href": "bbob/vis.html#functions-with-high-conditioning-and-unimodal",
    "title": "Visualizations of problem landscapes",
    "section": "Functions with high conditioning and unimodal",
    "text": "Functions with high conditioning and unimodal"
  },
  {
    "objectID": "bbob/vis.html#multi-modal-functions-with-adequate-global-structure",
    "href": "bbob/vis.html#multi-modal-functions-with-adequate-global-structure",
    "title": "Visualizations of problem landscapes",
    "section": "Multi-modal functions with adequate global structure",
    "text": "Multi-modal functions with adequate global structure"
  },
  {
    "objectID": "bbob/vis.html#multi-modal-functions-with-weak-global-structure",
    "href": "bbob/vis.html#multi-modal-functions-with-weak-global-structure",
    "title": "Visualizations of problem landscapes",
    "section": "Multi-modal functions with weak global structure",
    "text": "Multi-modal functions with weak global structure"
  },
  {
    "objectID": "bbob-noisy/vis.html",
    "href": "bbob-noisy/vis.html",
    "title": "Visualizations of problem landscapes",
    "section": "",
    "text": "Show plots in −  + columns (click on Dimension/Function/Instance/Visualization type below to show all plots for the chosen category)\n\n\n\n\nDimension\n\n\nFunction\n\n\nInstance\n\n\nVisualization type\n\n\n\n\n\n−\n\n +\n\n\n\n−\n\n +\n\n\n\n−\n\n +"
  },
  {
    "objectID": "bbob-noisy/vis.html#plot-explanation",
    "href": "bbob-noisy/vis.html#plot-explanation",
    "title": "Visualizations of problem landscapes",
    "section": "Plot explanation",
    "text": "Plot explanation\n\n\nSearch space cuts\nThe plots with search space cuts show the function value f along various lines in the search space that go through the global optimum \\mathbf{x}_\\mathrm{opt}. The colored lines change the value of only one variable x_i at a time keeping the rest fixed to \\mathbf{x}_\\mathrm{opt}. The gray line represents the line that goes through \\mathbf{x}_\\mathrm{opt} in the direction of the all-ones vector (i.e., in the diagonal direction). To improve visibility, only five colored lines are shown in the larger dimensions (D \\geq 10), corresponding to x_1, x_2, x_{\\lfloor D/2 \\rfloor}, x_{D-1} and x_D, where D is the search space dimension. The plots are shown in three variants:\n\nlin-lin: both axes are linear,\nlin-log: the x-axis is linear, the y-axis shows the difference between f and the optimal value f_\\mathrm{opt} on a logarithmic scale,\nlog-log: both axes are logarithmic, the x-axis shows the absolute difference to \\mathbf{x}_\\mathrm{opt} (positive directions presented as x_i and negative as -x_i), the y-axis shows the difference between f and f_\\mathrm{opt}.\n\n\n\n\n\nFunction value heatmap\nThe function value heatmap shows the function values f on a 2-D view of the search space that contains the optimal solution and is approximated by a grid. In addition to color-coded function values, the plots include level sets in gray hues. For dimensions larger than 2, the heatmaps of pairs of variables are organized into a matrix. To improve visibility, only five variables are included in the matrix in the larger dimensions (D \\geq 10), corresponding to x_1, x_2, x_{\\lfloor D/2 \\rfloor}, x_{D-1} and x_D, where D is the search space dimension.\n\n\n\n\nNormalized rank heatmap\nThe normalized rank heatmap shows, instead of absolute function values f, their normalized rank with 0 corresponding to the best rank and 1 to the worst one on a 2-D view of the search space that contains the optimal solution and is approximated by a grid. In addition to color-coded ranks, the plots include level sets in gray hues. For dimensions larger than 2, the heatmaps of pairs of variables are organized into a matrix. To improve visibility, only five variables are included in the matrix in the larger dimensions (D \\geq 10), corresponding to x_1, x_2, x_{\\lfloor D/2 \\rfloor}, x_{D-1} and x_D, where D is the search space dimension.\n\n\n\n\nSurface plot\nThe surface plot shows the function values f on a 3-D view of the search space and is available only for 2-D problems. To improve visibility, the z-axis is inverted, so that the global optimum is at the top of the plot."
  },
  {
    "objectID": "bbob-noisy/vis.html#problem-definition",
    "href": "bbob-noisy/vis.html#problem-definition",
    "title": "Visualizations of problem landscapes",
    "section": "Problem definition",
    "text": "Problem definition"
  },
  {
    "objectID": "bbob-noisy/vis.html#functions-with-moderate-noise",
    "href": "bbob-noisy/vis.html#functions-with-moderate-noise",
    "title": "Visualizations of problem landscapes",
    "section": "Functions with moderate noise",
    "text": "Functions with moderate noise\n\nSphere\nf_\\mathrm{sphere}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant"
  },
  {
    "objectID": "bbob-noisy/vis.html#functions-with-severe-noise",
    "href": "bbob-noisy/vis.html#functions-with-severe-noise",
    "title": "Visualizations of problem landscapes",
    "section": "Functions with severe noise",
    "text": "Functions with severe noise\n\nSphere\nf_\\mathrm{sphere}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant"
  },
  {
    "objectID": "bbob-noisy/vis.html#highly-multi-modal-functions-with-severe-noise",
    "href": "bbob-noisy/vis.html#highly-multi-modal-functions-with-severe-noise",
    "title": "Visualizations of problem landscapes",
    "section": "Highly multi-modal functions with severe noise",
    "text": "Highly multi-modal functions with severe noise\n\nSchaffer’s F7\nf_\\mathrm{schaffer}(\\mathbf{x}) = \\left(\\frac{1}{D- 1}\\sum_{i = 1}^{D- 1} \\sqrt{s_i} +\n      \\sqrt{s_i} \\sin^2\\!\\left(50\\,s_i^{1/5}\\right)\\right)^2\n\n\\mathbf{z}= \\Lambda^{\\!10}\\mathbf{Q}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\ns_i = \\sqrt{z_i^2 + z_{i+1}^2} for i=1,\\dots,D\n\nProperties:\nA highly multimodal function where frequency and amplitude of the modulation vary.\n\nconditioning is low"
  },
  {
    "objectID": "bbob-mixint/vis.html",
    "href": "bbob-mixint/vis.html",
    "title": "Visualizations of problem landscapes",
    "section": "",
    "text": "Plots\nShow plots in −  + columns (click on Dimension/Function/Instance/Visualization type below to show all plots for the chosen category)\n\n\n\n\nDimension\n\n\nFunction\n\n\nInstance\n\n\nVisualization type\n\n\n\n\n\n−\n\n\n\n\n\n+\n\n\n\n\n−\n\n +\n\n\n\n−\n\n +"
  },
  {
    "objectID": "bbob-mixint/def.html",
    "href": "bbob-mixint/def.html",
    "title": "Function definitions",
    "section": "",
    "text": "The bbob-mixint suite is constructed by partially discretizing problems from the bbob (Finck et al. 2009) and bbob-largescale (Varelas et al. 2018) suites. In the following, we first explain how the discretization is performed, then describe the construction of the suite and finally show how the functions are scaled to adjust their difficulty.\n\n\nConsider a bbob(or bbob-largescale) problem with the function f: \\mathbb{R}^n \\to \\mathbb{R} and optimal value f^{\\textrm{opt}} = f(\\mathbf{x}^{\\textrm{opt}}). The resulting mixed-integer function will have the form \\overline{f}: \\mathbb{Z}^k \\times \\mathbb{R}^{(n-k)} \\to \\mathbb{R}, that is, it will be defined on k integer and n-k continuous variables. While all bbob functions are defined for any \\mathbf{x} \\in \\mathbb{R}^n, all but the linear slope function have their optimal solution within [-4, 4]^n. The partial discretization is performed in such a way that the optimal value is preserved, that is \\vphantom{f}\\smash{\\overline f}^{\\textrm{opt}} = f^{\\textrm{opt}}.\nNow let us assume that we wish to discretize the variable x_i, where i \\in \\{1, \\dots, k\\}, into the set \\{0, \\dots, l-1\\} of l integer values. This discretization is done as follows:\n\n First, we define a sequence of l equidistant auxiliary values -4 &lt; y_1 &lt; \\cdots &lt; y_l &lt; 4 so that y_{j+1}-y_j=\\frac{4+4}{l + 1}=y_1-(-4)=4-y_l, where j = 1, \\dots, l - 1.\nWe denote with y^{*} the value y_j, j = 1, \\dots, l, that is closest to x_i^{\\text{opt}}. The difference between the two is represented by d_i = y^{*} - x_i^{\\text{opt}}. Note that |d_i| \\leq \\frac{4}{l+1} if x_i^{\\text{opt}} \\in [-4, 4].\nThen, z_j = y_j - d_i for j = 1, \\dots, l. This aligns one of the z_j values with x_i^{\\text{opt}}.\n Finally, the following transformation \\zeta is used to map any continuous value x_i \\in \\mathbb{R} to an integer in \\{0, \\dots, l-1\\}: \\zeta (x_i) =\n          \\left\\{\\begin{array}{cl}\n            0 &\\quad\\text{if}\\quad x_i &lt; z_1 + \\frac{4}{l+1} \\\\[0.2em]\n            1 &\\quad\\text{if}\\quad z_1 + \\frac{4}{l+1} \\leq x_i &lt; z_2 + \\frac{4}{l+1} \\\\\n            \\vdots & \\quad\\vdots \\\\\n            l-1 &\\quad\\text{if}\\quad z_{l-1} + \\frac{4}{l+1} \\leq x_i\n           \\end{array}\\right.\n\nThe values y_j, j = 1, \\dots, l, in Step (1) are chosen in such a way that the corresponding shifted values z_j remain within [-4, 4] if x_i^{\\text{opt}} is also in [-4, 4]. If not, the shift is larger, but z_j, j = 1, \\dots, l, never go beyond x_i^{\\text{opt}}, which in practice means they remain within [-5, 5]^n—the region of interest for all bbob problems.\n\n\n\nThe bbob suite consists of problems with 24 different functions in 6 dimensions, n = 2, 3, 5, 10, 20, 40, and 15 instances (see (Finck et al. 2009) for the function definitions). Because the discretization reduces the number of continuous variables, higher dimensions are used for the bbob-mixint suite to produce challenging problems. We chose n = 5, 10, 20, 40, 80, 160 as the dimensions of the bbob-mixint suite.1\nBecause the numerical effort for some bbob problems scales with n^2, we use these for dimensions \\leq 40 only. For dimensions &gt;40, we use the corresponding problems from the bbob-largescale suite (Varelas et al. 2018) which scale linearly with n.\nAs all dimensions n are multiples of 5, we define five arities for n/5 consecutive variables, respectively, as l=2,4,8,16,\\infty. We use instances 1–15 to instantiate each problem. They match the equally-numbered instances of the underlying bbob and bbob-largescale problems.\n\n\n\nInitial experiments using the algorithms Random Search, CMA-ES (Hansen and Auger 2014) and DE (Storn and Price 1997) have shown that the new problems are of considerably different difficulties. Some are extremely hard to solve, while for others, a non-negligible percentage of targets is met already after a handful of function evaluations. Because COCO’s performance assessment aggregates results over function and target pairs, we scale function values to adjust for these different difficulties.\nIn order to decide on the scaling factors, we look at how many targets can be reached just by evaluating the domain middle (often the first guess of an optimization algorithm). However, because two values could be interpreted as the ‘middle’ value for variables of even arity, we need to sample among a large set of possible domain middle points. Figure 1 (b) shows the difference between the median f-value of 1000 domain middle samples and the f-value of the optimal solution for each problem instance in the bbob-mixint suite prior to scaling. In comparison, Figure 1 (a) shows the difference between the f-value of the domain middle and of the optimal solution for each problem instance for the bbob suite (note that no sampling is required here since it is clear which point is the domain middle in a continuous domain).\nKeeping in mind that in COCO the easiest target defaults to 100, we see that for a number of bbob-mixint functions (f_2, f_6, f_{10} to f_{13} and f_{20}), the domain middle rarely (if ever) achieves this target. On the other hand, for functions such as f_{17}, f_{19} and f_{23}, evaluating the domain middle already guarantees reaching targets of 10 and less. We also see that the distances for the bbob-mixint suite are very similar to those for the bbob suite, albeit a bit larger in general. Based on these findings and the preliminary algorithm results, we choose to multiply the f-values of the functions with the scaling factors shown in Table. This setting is mindful of the connections between some functions, for example, the same scaling factors are used for the original (f_8) and rotated (f_9) Rosenbrock functions. Figure 1 (c) shows the result for all (scaled) bbob-mixint problems. Now the f-differences between the domain middle and the optimal solution are more uniform across the problems in the suite.\n\n\n\n\n\n\n\n(a) bbob suite\n\n\n\n\n\n\n\n(b) bbob-mixint suite before scaling\n\n\n\n\n\n\n\n(c) bbob-mixint suite\n\n\n\n\nFigure 1: Estimation of targets reached by the domain middle (in logarithmic scale). They are computed as the distance between the f -value of the domain middle and the optimal solution for the bbob suite (a), or the median of the distances between the f -values of 1000 domain middle samples and the optimal solution for the bbob-mixint suite before (b) and after (c) scaling. Each circle depicts one problem instance for instances 1–15.\n\n\n\nTable 1: Factors used for scaling the bbob-mixint functions.\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nValue\nFactor\nValue\nFactor\nValue\nFactor\nValue\n\n\n\n\nf_1\n1\nf_7\n1\nf_{13}\n0.1\nf_{19}\n10\n\n\nf_2\n10^{-3}\nf_8\n10^{-2}\nf_{14}\n1\nf_{20}\n0.1\n\n\nf_3\n0.1\nf_9\n10^{-2}\nf_{15}\n0.1\nf_{21}\n1\n\n\nf_4\n0.1\nf_{10}\n10^{-3}\nf_{16}\n1\nf_{22}\n1\n\n\nf_5\n1\nf_{11}\n10^{-2}\nf_{17}\n10\nf_{23}\n10\n\n\nf_6\n10^{-2}\nf_{12}\n10^{-4}\nf_{18}\n1\nf_{24}\n0.1\n\n\n\nTo summarize, the bbob-mixint suite contains mixed-integer problems constructed by discretizing the continuous problems from the bbob and bbob-largescale suites. Using 24 functions, 6 dimensions and 15 instances results in the total of 2160 problem instances."
  },
  {
    "objectID": "bbob-mixint/def.html#the-bbob-mixint-suite",
    "href": "bbob-mixint/def.html#the-bbob-mixint-suite",
    "title": "Function definitions",
    "section": "",
    "text": "The bbob-mixint suite is constructed by partially discretizing problems from the bbob (Finck et al. 2009) and bbob-largescale (Varelas et al. 2018) suites. In the following, we first explain how the discretization is performed, then describe the construction of the suite and finally show how the functions are scaled to adjust their difficulty.\n\n\nConsider a bbob(or bbob-largescale) problem with the function f: \\mathbb{R}^n \\to \\mathbb{R} and optimal value f^{\\textrm{opt}} = f(\\mathbf{x}^{\\textrm{opt}}). The resulting mixed-integer function will have the form \\overline{f}: \\mathbb{Z}^k \\times \\mathbb{R}^{(n-k)} \\to \\mathbb{R}, that is, it will be defined on k integer and n-k continuous variables. While all bbob functions are defined for any \\mathbf{x} \\in \\mathbb{R}^n, all but the linear slope function have their optimal solution within [-4, 4]^n. The partial discretization is performed in such a way that the optimal value is preserved, that is \\vphantom{f}\\smash{\\overline f}^{\\textrm{opt}} = f^{\\textrm{opt}}.\nNow let us assume that we wish to discretize the variable x_i, where i \\in \\{1, \\dots, k\\}, into the set \\{0, \\dots, l-1\\} of l integer values. This discretization is done as follows:\n\n First, we define a sequence of l equidistant auxiliary values -4 &lt; y_1 &lt; \\cdots &lt; y_l &lt; 4 so that y_{j+1}-y_j=\\frac{4+4}{l + 1}=y_1-(-4)=4-y_l, where j = 1, \\dots, l - 1.\nWe denote with y^{*} the value y_j, j = 1, \\dots, l, that is closest to x_i^{\\text{opt}}. The difference between the two is represented by d_i = y^{*} - x_i^{\\text{opt}}. Note that |d_i| \\leq \\frac{4}{l+1} if x_i^{\\text{opt}} \\in [-4, 4].\nThen, z_j = y_j - d_i for j = 1, \\dots, l. This aligns one of the z_j values with x_i^{\\text{opt}}.\n Finally, the following transformation \\zeta is used to map any continuous value x_i \\in \\mathbb{R} to an integer in \\{0, \\dots, l-1\\}: \\zeta (x_i) =\n          \\left\\{\\begin{array}{cl}\n            0 &\\quad\\text{if}\\quad x_i &lt; z_1 + \\frac{4}{l+1} \\\\[0.2em]\n            1 &\\quad\\text{if}\\quad z_1 + \\frac{4}{l+1} \\leq x_i &lt; z_2 + \\frac{4}{l+1} \\\\\n            \\vdots & \\quad\\vdots \\\\\n            l-1 &\\quad\\text{if}\\quad z_{l-1} + \\frac{4}{l+1} \\leq x_i\n           \\end{array}\\right.\n\nThe values y_j, j = 1, \\dots, l, in Step (1) are chosen in such a way that the corresponding shifted values z_j remain within [-4, 4] if x_i^{\\text{opt}} is also in [-4, 4]. If not, the shift is larger, but z_j, j = 1, \\dots, l, never go beyond x_i^{\\text{opt}}, which in practice means they remain within [-5, 5]^n—the region of interest for all bbob problems.\n\n\n\nThe bbob suite consists of problems with 24 different functions in 6 dimensions, n = 2, 3, 5, 10, 20, 40, and 15 instances (see (Finck et al. 2009) for the function definitions). Because the discretization reduces the number of continuous variables, higher dimensions are used for the bbob-mixint suite to produce challenging problems. We chose n = 5, 10, 20, 40, 80, 160 as the dimensions of the bbob-mixint suite.1\nBecause the numerical effort for some bbob problems scales with n^2, we use these for dimensions \\leq 40 only. For dimensions &gt;40, we use the corresponding problems from the bbob-largescale suite (Varelas et al. 2018) which scale linearly with n.\nAs all dimensions n are multiples of 5, we define five arities for n/5 consecutive variables, respectively, as l=2,4,8,16,\\infty. We use instances 1–15 to instantiate each problem. They match the equally-numbered instances of the underlying bbob and bbob-largescale problems.\n\n\n\nInitial experiments using the algorithms Random Search, CMA-ES (Hansen and Auger 2014) and DE (Storn and Price 1997) have shown that the new problems are of considerably different difficulties. Some are extremely hard to solve, while for others, a non-negligible percentage of targets is met already after a handful of function evaluations. Because COCO’s performance assessment aggregates results over function and target pairs, we scale function values to adjust for these different difficulties.\nIn order to decide on the scaling factors, we look at how many targets can be reached just by evaluating the domain middle (often the first guess of an optimization algorithm). However, because two values could be interpreted as the ‘middle’ value for variables of even arity, we need to sample among a large set of possible domain middle points. Figure 1 (b) shows the difference between the median f-value of 1000 domain middle samples and the f-value of the optimal solution for each problem instance in the bbob-mixint suite prior to scaling. In comparison, Figure 1 (a) shows the difference between the f-value of the domain middle and of the optimal solution for each problem instance for the bbob suite (note that no sampling is required here since it is clear which point is the domain middle in a continuous domain).\nKeeping in mind that in COCO the easiest target defaults to 100, we see that for a number of bbob-mixint functions (f_2, f_6, f_{10} to f_{13} and f_{20}), the domain middle rarely (if ever) achieves this target. On the other hand, for functions such as f_{17}, f_{19} and f_{23}, evaluating the domain middle already guarantees reaching targets of 10 and less. We also see that the distances for the bbob-mixint suite are very similar to those for the bbob suite, albeit a bit larger in general. Based on these findings and the preliminary algorithm results, we choose to multiply the f-values of the functions with the scaling factors shown in Table. This setting is mindful of the connections between some functions, for example, the same scaling factors are used for the original (f_8) and rotated (f_9) Rosenbrock functions. Figure 1 (c) shows the result for all (scaled) bbob-mixint problems. Now the f-differences between the domain middle and the optimal solution are more uniform across the problems in the suite.\n\n\n\n\n\n\n\n(a) bbob suite\n\n\n\n\n\n\n\n(b) bbob-mixint suite before scaling\n\n\n\n\n\n\n\n(c) bbob-mixint suite\n\n\n\n\nFigure 1: Estimation of targets reached by the domain middle (in logarithmic scale). They are computed as the distance between the f -value of the domain middle and the optimal solution for the bbob suite (a), or the median of the distances between the f -values of 1000 domain middle samples and the optimal solution for the bbob-mixint suite before (b) and after (c) scaling. Each circle depicts one problem instance for instances 1–15.\n\n\n\nTable 1: Factors used for scaling the bbob-mixint functions.\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nValue\nFactor\nValue\nFactor\nValue\nFactor\nValue\n\n\n\n\nf_1\n1\nf_7\n1\nf_{13}\n0.1\nf_{19}\n10\n\n\nf_2\n10^{-3}\nf_8\n10^{-2}\nf_{14}\n1\nf_{20}\n0.1\n\n\nf_3\n0.1\nf_9\n10^{-2}\nf_{15}\n0.1\nf_{21}\n1\n\n\nf_4\n0.1\nf_{10}\n10^{-3}\nf_{16}\n1\nf_{22}\n1\n\n\nf_5\n1\nf_{11}\n10^{-2}\nf_{17}\n10\nf_{23}\n10\n\n\nf_6\n10^{-2}\nf_{12}\n10^{-4}\nf_{18}\n1\nf_{24}\n0.1\n\n\n\nTo summarize, the bbob-mixint suite contains mixed-integer problems constructed by discretizing the continuous problems from the bbob and bbob-largescale suites. Using 24 functions, 6 dimensions and 15 instances results in the total of 2160 problem instances."
  },
  {
    "objectID": "bbob-mixint/def.html#footnotes",
    "href": "bbob-mixint/def.html#footnotes",
    "title": "Function definitions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the function definitions of all mentioned test suites are scalable in dimension. The six dimensions are only pre-chosen to facilitate the experimental setup.↩︎"
  },
  {
    "objectID": "bbob-noisy/def.html",
    "href": "bbob-noisy/def.html",
    "title": "Function definitions",
    "section": "",
    "text": "Used symbols and definitions of, e.g., auxiliary functions are given in the following. Vectors are typeset in bold and refer to column vectors.\n\\otimes : indicates element-wise multiplication of two D-dimensional vectors, \\otimes:\\mathcal{R}^D\\times\\mathcal{R}^D\\to\\mathcal{R}^D,  (\\mathbf{x},\\mathbf{y})\\mapsto\\mathrm{{diag}}(\\mathbf{x})\\times\\mathbf{y}=(x_i\\times y_i)_{i=1,\\dots,D}\n\\|.\\| : denotes the Euclidean norm, \\|\\mathbf{x}\\|^2=\\sum_i x_i^2.\n[.] : denotes the nearest integer value\n\\mathbf{0} : =(0,\\dots,0)^{\\mathrm{T}} all zero vector\n\\mathbf{1} : =(1,\\dots,1)^{\\mathrm{T}} all one vector\n\\Lambda^{\\!\\alpha} : is a diagonal matrix in D dimensions with the ith diagonal element as \\lambda_{ii} = \\alpha^{\\frac{1}{2}\\frac{i-1}{D-1}}\nf^{{}}_\\mathrm{pen} : :\\mathcal{R}^D\\to\\mathcal{R}, \\mathbf{x}\\mapsto 100\\sum_{i=1}^D\\max(0,|x_i| - 5)^2\n\\mathbf{1}_-^+ : a D-dimensional vector with entries of -1 or 1 both drawn equal probability.\n\\mathbf{Q}, \\mathbf{R} : orthogonal (rotation) matrices. For each function instance in each dimension a single realization for respectively \\mathbf{Q} and \\mathbf{R} is used. Rotation matrices are generated from standard normally distributed entries by Gram-Schmidt orthogonalization. Columns and rows of a rotation matrix form an orthogonal basis.\n\\mathbf{R} : see \\mathbf{Q}\nT^{{\\beta}_\\mathrm{asy}} : :\\mathcal{R}^D\\to\\mathcal{R}^D, x_i\\mapsto  \\begin{cases}  x_i^{1+\\beta \\frac{i-1}{D-1}\\sqrt{x_i}} & \\text{~if~} x_i&gt;0\\\\  x_i & \\text{~otherwise~}  \\end{cases}, for i=1,\\dots,D.\nT_\\mathrm{\\hspace*{-0.01em}osz} : :\\mathcal{R}^n\\to\\mathcal{R}^n, for any positive integer n, maps element-wise x\\mapsto\\mathrm{{sign}}(x)\\exp\\left(\\hat{x} +\n       0.049\\left(\\sin(c_1 \\hat{x}) + \\sin(c_2 \\hat{x})\\right)\\right) with \\hat{x}= \\begin{cases}  \\log(|x|) & \\text{if~} x\\not=0 \\\\  \\in\\mathcal{R}& \\text{otherwise}  \\end{cases}, \\mathrm{{sign}}(x) = \\begin{cases} -1 & \\text{if~} x &lt; 0 \\\\  0 & \\text{if~} x = 0 \\\\  1 & \\text{otherwise}  \\end{cases}, c_1 = \\begin{cases}  10 & \\text{if~} x &gt; 0\\\\  5.5 & \\text{otherwise}  \\end{cases} and c_2 = \\begin{cases}  7.9 & \\text{if~} x &gt; 0\\\\  3.1 & \\text{otherwise}  \\end{cases}\n\\mathbf{x}^\\mathrm{opt} : optimal solution vector, such that f(\\mathbf{x^\\mathrm{opt}}) is minimal."
  },
  {
    "objectID": "bbob-noisy/def.html#symbols-and-definitions",
    "href": "bbob-noisy/def.html#symbols-and-definitions",
    "title": "Function definitions",
    "section": "",
    "text": "Used symbols and definitions of, e.g., auxiliary functions are given in the following. Vectors are typeset in bold and refer to column vectors.\n\\otimes : indicates element-wise multiplication of two D-dimensional vectors, \\otimes:\\mathcal{R}^D\\times\\mathcal{R}^D\\to\\mathcal{R}^D,  (\\mathbf{x},\\mathbf{y})\\mapsto\\mathrm{{diag}}(\\mathbf{x})\\times\\mathbf{y}=(x_i\\times y_i)_{i=1,\\dots,D}\n\\|.\\| : denotes the Euclidean norm, \\|\\mathbf{x}\\|^2=\\sum_i x_i^2.\n[.] : denotes the nearest integer value\n\\mathbf{0} : =(0,\\dots,0)^{\\mathrm{T}} all zero vector\n\\mathbf{1} : =(1,\\dots,1)^{\\mathrm{T}} all one vector\n\\Lambda^{\\!\\alpha} : is a diagonal matrix in D dimensions with the ith diagonal element as \\lambda_{ii} = \\alpha^{\\frac{1}{2}\\frac{i-1}{D-1}}\nf^{{}}_\\mathrm{pen} : :\\mathcal{R}^D\\to\\mathcal{R}, \\mathbf{x}\\mapsto 100\\sum_{i=1}^D\\max(0,|x_i| - 5)^2\n\\mathbf{1}_-^+ : a D-dimensional vector with entries of -1 or 1 both drawn equal probability.\n\\mathbf{Q}, \\mathbf{R} : orthogonal (rotation) matrices. For each function instance in each dimension a single realization for respectively \\mathbf{Q} and \\mathbf{R} is used. Rotation matrices are generated from standard normally distributed entries by Gram-Schmidt orthogonalization. Columns and rows of a rotation matrix form an orthogonal basis.\n\\mathbf{R} : see \\mathbf{Q}\nT^{{\\beta}_\\mathrm{asy}} : :\\mathcal{R}^D\\to\\mathcal{R}^D, x_i\\mapsto  \\begin{cases}  x_i^{1+\\beta \\frac{i-1}{D-1}\\sqrt{x_i}} & \\text{~if~} x_i&gt;0\\\\  x_i & \\text{~otherwise~}  \\end{cases}, for i=1,\\dots,D.\nT_\\mathrm{\\hspace*{-0.01em}osz} : :\\mathcal{R}^n\\to\\mathcal{R}^n, for any positive integer n, maps element-wise x\\mapsto\\mathrm{{sign}}(x)\\exp\\left(\\hat{x} +\n       0.049\\left(\\sin(c_1 \\hat{x}) + \\sin(c_2 \\hat{x})\\right)\\right) with \\hat{x}= \\begin{cases}  \\log(|x|) & \\text{if~} x\\not=0 \\\\  \\in\\mathcal{R}& \\text{otherwise}  \\end{cases}, \\mathrm{{sign}}(x) = \\begin{cases} -1 & \\text{if~} x &lt; 0 \\\\  0 & \\text{if~} x = 0 \\\\  1 & \\text{otherwise}  \\end{cases}, c_1 = \\begin{cases}  10 & \\text{if~} x &gt; 0\\\\  5.5 & \\text{otherwise}  \\end{cases} and c_2 = \\begin{cases}  7.9 & \\text{if~} x &gt; 0\\\\  3.1 & \\text{otherwise}  \\end{cases}\n\\mathbf{x}^\\mathrm{opt} : optimal solution vector, such that f(\\mathbf{x^\\mathrm{opt}}) is minimal."
  },
  {
    "objectID": "bbob-noisy/def.html#general-setup",
    "href": "bbob-noisy/def.html#general-setup",
    "title": "Function definitions",
    "section": "General Setup",
    "text": "General Setup\n\nSearch Space\nAll functions are defined and can be evaluated over \\mathcal{R}^{D}, while the actual search domain is given as [-5,5]^{D}.\n\n\nLocation of the optimal \\mathbf{x}^\\mathrm{opt} and of f_\\mathrm{opt}=f(\\mathbf{x^\\mathrm{opt}})\nAll functions have their global optimum in [-5,5]^{D}. The majority of functions has the global optimum in [-4,4]^{D}. The value for f_\\mathrm{opt} is drawn from a cauchy distributed random variable, with roughly 50% of the values between -100 and 100. The value is rounded after two decimal places and the maximum and minimum are set to 1000 and -1000 respectively. In the function definitions a transformed variable vector \\mathbf{z} is often used instead of the argument \\mathbf{x}. The vector \\mathbf{z} has its optimum in \\mathbf{z^\\mathrm{opt}}=\\mathbf{0}, if not stated otherwise.\n\n\nBoundary Handling\nOn all functions a penalty boundary handling is applied as given with f^{{}}_\\mathrm{pen}.\n\n\nLinear Transformations\nLinear transformations of the search space are applied to derive non-separable functions from separable ones and to control the conditioning of the function.\n\n\nNon-Linear Transformations and Symmetry Breaking\nIn order to make relatively simple, but well understood functions less regular, on some functions non-linear transformations are applied in x- or f-space. Both transformations T_\\mathrm{\\hspace*{-0.01emosz}}:\\mathcal{R}^n\\to\\mathcal{R}^n, n\\in\\{1,D\\}, and T^{{}_\\mathrm{asy}}:\\mathcal{R}^D\\to\\mathcal{R}^D are defined coordinate-wise. They are smooth and have, coordinate-wise, a strictly positive derivative. T_\\mathrm{\\hspace*{-0.01emosz}} is oscillating about the identity, where the oscillation is scale invariant w.r.t. the origin. T^{{}}_\\mathrm{asy} is the identity for negative values. When T^{{}}_\\mathrm{asy} is applied, a portion of 1/2^D of the search space remains untransformed."
  },
  {
    "objectID": "bbob-noisy/def.html#noise-models",
    "href": "bbob-noisy/def.html#noise-models",
    "title": "Function definitions",
    "section": "Noise Models",
    "text": "Noise Models\nIn this benchmarking suite three different noise models are used. The first two, f_{\\mathrm{GN}}\\left({}\\right) and f_{\\mathrm{UN}}\\left({}\\right), are multiplicative noise models while the third model, f_{\\mathrm{CN}}\\left({}\\right), is an additive noise model. All noise models are applied to a function value f under the assumption that f\\ge0. All noise models reveal stochastic dominance between any two solutions and are therefore utility-free.\n\nGaussian Noise\nThe Gaussian noise model is scale invariant and defined as f_{\\mathrm{GN}\\left({f,\\beta}\\right)} = f\\times\\exp(\\beta\\,\\mathcal{N}(0,1))\\enspace. The noise strength is controlled with \\beta. The distribution of the noise is log-normal, thus no negative noise values can be sampled. For the benchmark functions with moderate noise \\beta = 0.01, otherwise \\beta = 1. For small values of \\beta this noise model resembles f\\times(1+\\beta\\,\\mathcal{N}(0,1)).\n\n\nUniform Noise\nThe uniform noise model is introduced as a more severe noise model than the Gaussian and is defined as \n  f_{\\mathrm{UN}\\left({f,\\alpha,\\beta}\\right)} = f\\times\\mathcal{U}(0,1)^{\\beta}\\max\\left(1, \\left(\\dfrac{10^9}{f + \\epsilon}\\right)^{\\alpha\\,\\mathcal{U}(0,1)}\\right)\\enspace. The noise model uses two random factors. The first factor is in the interval [0,1], uniformly distributed for \\beta=1. The second factor, \\max(\\dots), is \\ge1. The parameters \\alpha and \\beta control the noise strength. For moderate noise \\alpha = 0.01\\left(0.49 + {1}/{D}\\right) and \\beta = 0.01, otherwise \\alpha = 0.49 + {1}/{D} and \\beta = 1. Furthermore, \\epsilon is set to 10^{-99} in order to prevent division by zero.\nThe uniform noise model is not scale invariant. Due to the last factor the noise strength increases with decreasing (positive) value of f. Therefore the noise strength becomes more severe when approaching the optimum.\n\n\nCauchy Noise\nThe Cauchy noise model represents a different type of noise with two important aspects. First, only a comparatively small percentage of function values is disturbed by noise. Second, the noise distribution is comparatively “weird”. Large outliers occur once in a while, and because they stem from a continuous distribution they cannot be easily diagnosed. The Cauchy noise model is defined as f_{\\mathrm{CN}\\left({f,\\alpha,p}\\right)} = f + \\alpha\\,\\max\\left(0, 1000 + \\mathbb{I}_{\\left\\{\\mathcal{U}(0,1) &lt; p\\right\\}}\\dfrac{\\mathcal{N}(0,1)}{|\\mathcal{N}(0,1)|+\\epsilon}\\right)\\enspace, where \\alpha defines the noise strength and p determines the frequency of the noise disturbance. In the moderate noise case \\alpha = 0.01 and p = 0.05, otherwise \\alpha = 1 and p = 0.2. The summand of 1000 was chosen to sample positive and negative “outliers” (as the function value is cut from below, see next paragraph) and \\epsilon is set to 10^{-199}.\n\n\nFinal Function Value\nIn order to achieve a convenient testing for the target function value, in all noise models 1.01\\times10^{-8} is added to the function value and, if the input argument f is smaller than 10^{-8}, the undisturbed f is returned.\nf_\\mathrm{XX}(f,\\dots) \\gets \\begin{cases}f_\\mathrm{XX}(f,\\dots) + 1.01\\times10^{-8}& \\text{if~} f\\ge10^{-8}\\\\\n                                 f & \\text{otherwise}\n\\end{cases}"
  },
  {
    "objectID": "bbob-noisy/def.html#functions-with-moderate-noise",
    "href": "bbob-noisy/def.html#functions-with-moderate-noise",
    "title": "Function definitions",
    "section": "Functions with moderate noise",
    "text": "Functions with moderate noise\n\nSphere\nf_\\mathrm{sphere}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant"
  },
  {
    "objectID": "bbob-noisy/def.html#functions-with-severe-noise",
    "href": "bbob-noisy/def.html#functions-with-severe-noise",
    "title": "Function definitions",
    "section": "Functions with severe noise",
    "text": "Functions with severe noise\n\nSphere\nf_\\mathrm{sphere}(\\mathbf{x}) = \\|\\mathbf{z}\\|^2\n\n\\mathbf{z}= \\mathbf{x}- \\mathbf{x^\\mathrm{opt}}\n\nProperties:\nPresumably the most easy continuous domain search problem, given the volume of the searched solution is small (i.e. where pure monte-carlo random search is too expensive).\n\nunimodal\nhighly symmetric, in particular rotationally invariant"
  },
  {
    "objectID": "bbob-noisy/def.html#highly-multi-modal-functions-with-severe-noise",
    "href": "bbob-noisy/def.html#highly-multi-modal-functions-with-severe-noise",
    "title": "Function definitions",
    "section": "Highly multi-modal functions with severe noise",
    "text": "Highly multi-modal functions with severe noise\n\nSchaffer’s F7\nf_\\mathrm{schaffer}(\\mathbf{x}) = \\left(\\frac{1}{D- 1}\\sum_{i = 1}^{D- 1} \\sqrt{s_i} +\n      \\sqrt{s_i} \\sin^2\\!\\left(50\\,s_i^{1/5}\\right)\\right)^2\n\n\\mathbf{z}= \\Lambda^{\\!10}\\mathbf{Q}\\,T^{{0.5}}_\\mathrm{asy}(\\mathbf{R}(\\mathbf{x}-\\mathbf{x^\\mathrm{opt}}))\ns_i = \\sqrt{z_i^2 + z_{i+1}^2} for i=1,\\dots,D\n\nProperties:\nA highly multimodal function where frequency and amplitude of the modulation vary.\n\nconditioning is low"
  },
  {
    "objectID": "bbob/def.html",
    "href": "bbob/def.html",
    "title": "Function definitions",
    "section": "",
    "text": "This is an online-friendly presentation of the bbob functions, copied from the BBOB 2009 function document (Finck et al. 2009). You may cite this work in a scientific context as\n\nSteffen Finck, Nikolaus Hansen, Raymond Ros, and Anne Auger. Real-Parameter Black-Box Optimization Benchmarking 2009: Noiseless Functions Definitions. Technical report, RR-6829. INRIA, 2009. https://inria.hal.science/inria-00362633v2/document\n\n@techreport{bbob2019,\n    author = {Finck, Steffen and Hansen, Nikolaus and Ros, Raymond and Auger, Anne},\n    title = {Real-Parameter Black-Box Optimization Benchmarking 2009: Noiseless Functions Definitions},\n    institution = {INRIA},\n    year = {2009},\n    number = {RR-6829},\n    note = {Updated version as of February 2019},\n    url = {https://inria.hal.science/inria-00362633v2/document}\n}\nIn the following, 24 noise-free real-parameter single-objective benchmark functions are presented1. Our intention behind the selection of benchmark functions was to evaluate the performance of algorithms with regard to typical difficulties which we believe occur in continuous domain search. We hope that the function collection reflects, at least to a certain extend and with a few exceptions, a more difficult portion of the problem distribution that will be seen in practice (easy functions are evidently of lesser interest).\nWe prefer benchmark functions that are comprehensible such that algorithm behaviours can be understood in the topological context. In this way, a desired search behaviour can be pictured and deficiencies of algorithms can be profoundly analysed. Last but not least, this can eventually lead to a systematic improvement of algorithms.\nAll benchmark functions are scalable with the dimension. Most functions have no specific value of their optimal solution (they are randomly shifted in x-space). All functions have an artificially chosen optimal function value (they are randomly shifted in f-space). Consequently, for each function different instances can be generated: for each instance the randomly chosen values are drawn anew2. Apart from the first subgroup, the benchmarks are non-separable. Other specific properties are discussed in Function Properties."
  },
  {
    "objectID": "bbob/def.html#introduction",
    "href": "bbob/def.html#introduction",
    "title": "Function definitions",
    "section": "",
    "text": "This is an online-friendly presentation of the bbob functions, copied from the BBOB 2009 function document (Finck et al. 2009). You may cite this work in a scientific context as\n\nSteffen Finck, Nikolaus Hansen, Raymond Ros, and Anne Auger. Real-Parameter Black-Box Optimization Benchmarking 2009: Noiseless Functions Definitions. Technical report, RR-6829. INRIA, 2009. https://inria.hal.science/inria-00362633v2/document\n\n@techreport{bbob2019,\n    author = {Finck, Steffen and Hansen, Nikolaus and Ros, Raymond and Auger, Anne},\n    title = {Real-Parameter Black-Box Optimization Benchmarking 2009: Noiseless Functions Definitions},\n    institution = {INRIA},\n    year = {2009},\n    number = {RR-6829},\n    note = {Updated version as of February 2019},\n    url = {https://inria.hal.science/inria-00362633v2/document}\n}\nIn the following, 24 noise-free real-parameter single-objective benchmark functions are presented1. Our intention behind the selection of benchmark functions was to evaluate the performance of algorithms with regard to typical difficulties which we believe occur in continuous domain search. We hope that the function collection reflects, at least to a certain extend and with a few exceptions, a more difficult portion of the problem distribution that will be seen in practice (easy functions are evidently of lesser interest).\nWe prefer benchmark functions that are comprehensible such that algorithm behaviours can be understood in the topological context. In this way, a desired search behaviour can be pictured and deficiencies of algorithms can be profoundly analysed. Last but not least, this can eventually lead to a systematic improvement of algorithms.\nAll benchmark functions are scalable with the dimension. Most functions have no specific value of their optimal solution (they are randomly shifted in x-space). All functions have an artificially chosen optimal function value (they are randomly shifted in f-space). Consequently, for each function different instances can be generated: for each instance the randomly chosen values are drawn anew2. Apart from the first subgroup, the benchmarks are non-separable. Other specific properties are discussed in Function Properties."
  },
  {
    "objectID": "bbob/def.html#general-setup",
    "href": "bbob/def.html#general-setup",
    "title": "Function definitions",
    "section": "General setup",
    "text": "General setup\nSearch Space:\nAll functions are defined and can be evaluated over \\mathcal{R}^{D}, while the actual search domain is given as [-5,5]^{D}.\nLocation of the optimal \\mathbf{x}^\\mathrm{opt} and of f_\\mathrm{opt}=f(\\mathbf{x^\\mathrm{opt}}):\nAll functions have their global optimum in [-5,5]^{D}. The majority of functions has the global optimum in [-4,4]^{D} and for many of them \\mathbf{x}^\\mathrm{opt} is drawn uniformly from this compact. The value for f_\\mathrm{opt} is drawn from a Cauchy distributed random variable, with zero median and with roughly 50% of the values between -100 and 100. The value is rounded after two decimal places and set to \\pm1000 if its absolute value exceeds 1000. In the function definitions a transformed variable vector \\mathbf{z} is often used instead of the argument \\mathbf{x}. The vector \\mathbf{z} has its optimum in \\mathbf{z^\\mathrm{opt}}=\\mathbf{0}, if not stated otherwise.\nBoundary Handling:\nOn some functions a penalty boundary handling is applied as given with f^{{}}_\\mathrm{pen} (see Symbols and definitions).\nLinear Transformations:\nLinear transformations of the search space are applied to derive non-separable functions from separable ones and to control the conditioning of the function.\nNon-Linear Transformations and Symmetry Breaking:\nIn order to make relatively simple, but well understood functions less regular, on some functions non-linear transformations are applied in x- or f-space. Both transformations T_\\mathrm{\\hspace*{-0.01emosz}}:\\mathcal{R}^n\\to\\mathcal{R}^n, n\\in\\{1,D\\}, and T_\\mathrm{asy}:\\mathcal{R}^D\\to\\mathcal{R}^D are defined coordinate-wise (see Symbols and definitions). They are smooth and have, coordinate-wise, a strictly positive derivative. They are shown in Figure 1. T_\\mathrm{osz} is oscillating about the identity, where the oscillation is scale invariant w.r.t. the origin. T^{{}}_\\mathrm{asy} is the identity for negative values. When T^{{}}_\\mathrm{asy} is applied, a portion of 1/2^D of the search space remains untransformed.\n\n\n\nFigure 1: T_\\mathrm{osz} (blue) and D-th coordinate of T_\\mathrm{asy} for \\beta = 0.1, 0.2, 0.5 (green)"
  },
  {
    "objectID": "bbob/def.html#symbols-and-definitions",
    "href": "bbob/def.html#symbols-and-definitions",
    "title": "Function definitions",
    "section": "Symbols and definitions",
    "text": "Symbols and definitions\nUsed symbols and definitions of, e.g., auxiliary functions are given in the following. Vectors are typeset in bold and refer to column vectors.\n\\otimes indicates element-wise multiplication of two D-dimensional vectors, \\otimes:\\mathcal{R}^D\\times\\mathcal{R}^D\\to\\mathcal{R}^D,  (\\mathbf{x},\\mathbf{y})\\mapsto\\mathrm{{diag}}(\\mathbf{x})\\times\\mathbf{y}=(x_i\\times y_i)_{i=1,\\dots,D}\n\\|.\\| denotes the Euclidean norm, \\|\\mathbf{x}\\|^2=\\sum_i x_i^2.\n[.] denotes the nearest integer value\n\\mathbf{0} =(0,\\dots,0)^{\\mathrm{T}} all zero vector\n\\mathbf{1} =(1,\\dots,1)^{\\mathrm{T}} all one vector\n\\Lambda^{\\!\\alpha} is a diagonal matrix in D dimensions with the ith diagonal element as \\lambda_{ii} =  \\alpha^{\\frac{1}{2}\\frac{i-1}{D-1}}, for i=1,\\dots,D.\nf^{{}}_\\mathrm{pen} :\\mathcal{R}^D\\to\\mathcal{R}, \\mathbf{x}\\mapsto\\sum_{i=1}^D\\max(0,|x_i| - 5)^2\n\\mathbf{1}_-^+ a D-dimensional vector with entries of -1 or 1 with equal probability independently drawn.\n\\mathbf{Q}, \\mathbf{R} orthogonal (rotation) matrices. For one function in one dimension a different realization for respectively \\mathbf{Q} and \\mathbf{R} is used for each instantiation of the function. Orthogonal matrices are generated from standard normally distributed entries by Gram-Schmidt orthonormalization. Columns and rows of an orthogonal matrix form an orthonormal basis.\n\\mathbf{R} see \\mathbf{Q}\nT^{{\\beta}}_\\mathrm{asy} :\\mathcal{R}^D\\to\\mathcal{R}^D, x_i\\mapsto  \\begin{cases}  x_i^{1+\\beta \\frac{i-1}{D-1}\\sqrt{x_i}} & \\text{~if~} x_i&gt;0\\\\  x_i & \\text{~otherwise~}  \\end{cases}, for i=1,\\dots,D. See Figure 1.\nT_\\mathrm{\\hspace*{-0.01em}osz} :\\mathcal{R}^n\\to\\mathcal{R}^n, for any positive integer n (n=1 and n=D are used in the following), maps element-wise x\\mapsto\\mathrm{{sign}}(x)\\exp\\left(\\hat{x} +\n       0.049\\left(\\sin(c_1 \\hat{x}) + \\sin(c_2 \\hat{x})\\right)\\right) with \\hat{x}= \\begin{cases}  \\log(|x|) & \\text{if~} x\\not=0 \\\\  0 & \\text{otherwise}  \\end{cases}, \\mathrm{{sign}}(x) = \\begin{cases} -1 & \\text{if~} x &lt; 0 \\\\  0 & \\text{if~} x = 0 \\\\  1 & \\text{otherwise}  \\end{cases}, c_1 = \\begin{cases}  10 & \\text{if~} x &gt; 0\\\\  5.5 & \\text{otherwise}  \\end{cases} and c_2 = \\begin{cases}  7.9 & \\text{if~} x &gt; 0\\\\  3.1 & \\text{otherwise}  \\end{cases}. See Figure 1.\n\\mathbf{x}^\\mathrm{opt} optimal solution vector, such that f(\\mathbf{x^\\mathrm{opt}}) is minimal."
  },
  {
    "objectID": "bbob/def.html#separable-functions",
    "href": "bbob/def.html#separable-functions",
    "title": "Function definitions",
    "section": "Separable functions",
    "text": "Separable functions"
  },
  {
    "objectID": "bbob/def.html#functions-with-low-or-moderate-conditioning",
    "href": "bbob/def.html#functions-with-low-or-moderate-conditioning",
    "title": "Function definitions",
    "section": "Functions with low or moderate conditioning",
    "text": "Functions with low or moderate conditioning"
  },
  {
    "objectID": "bbob/def.html#functions-with-high-conditioning-and-unimodal",
    "href": "bbob/def.html#functions-with-high-conditioning-and-unimodal",
    "title": "Function definitions",
    "section": "Functions with high conditioning and unimodal",
    "text": "Functions with high conditioning and unimodal"
  },
  {
    "objectID": "bbob/def.html#multi-modal-functions-with-adequate-global-structure",
    "href": "bbob/def.html#multi-modal-functions-with-adequate-global-structure",
    "title": "Function definitions",
    "section": "Multi-modal functions with adequate global structure",
    "text": "Multi-modal functions with adequate global structure"
  },
  {
    "objectID": "bbob/def.html#multi-modal-functions-with-weak-global-structure",
    "href": "bbob/def.html#multi-modal-functions-with-weak-global-structure",
    "title": "Function definitions",
    "section": "Multi-modal functions with weak global structure",
    "text": "Multi-modal functions with weak global structure"
  },
  {
    "objectID": "bbob/def.html#function-properties",
    "href": "bbob/def.html#function-properties",
    "title": "Function definitions",
    "section": "Function properties",
    "text": "Function properties\n\nDeceptive Functions\nAll “deceptive” functions provide, beyond their deceptivity, a “structure” that can be exploited to solve them in a reasonable procedure.\n\n\nIll-Conditioning\nIll-conditioning is a typical challenge in real-parameter optimization and, besides multimodality, probably the most common one. Conditioning of a function can be rigorously formalized in the case of convex quadratic functions, f({\\mathbf{x}}) =\\frac12 {\\mathbf{x}}^{T} {\\mathbf{H}}{\\mathbf{x}} where {\\mathbf{H}} is a symmetric definite positive matrix, as the condition number of the Hessian matrix {\\mathbf{H}}. Since contour lines associated to a convex quadratic function are ellipsoids, the condition number corresponds to the square root of the ratio between the largest axis of the ellipsoid and the shortest axis. For more general functions, conditioning loosely refers to the square of the ratio between the largest direction and smallest of a contour line. The testbed contains ill-conditioned functions with a typical conditioning of 10^6. We believe this a realistic requirement, while we have seen practical problems with conditioning as large as 10^{10}.\n\n\nRegularity\nFunctions from simple formulas are often highly regular. We have used a non-linear transformation, T_\\mathrm{\\hspace*{-0.01em}osz}, in order to introduce small, smooth but clearly visible irregularities. Furthermore, the testbed contains a few highly irregular functions.\n\n\nSeparability\nIn general, separable functions pose an essentially different search problem to solve, because the search process can be reduced to D one-dimensional search procedures. Consequently, non-separable problems must be considered much more difficult and most benchmark functions are designed being non-separable. The typical well-established technique to generate non-separable functions from separable ones is the application of a rotation matrix \\mathcal{R}.\n\n\nSymmetry\nStochastic search procedures often rely on Gaussian distributions to generate new solutions and it has been argued that symmetric benchmark functions could be in favor of these operators. To avoid a bias in favor of highly symmetric operators we have used a symmetry breaking transformation, T^{{}}_\\mathrm{asy}. We have also included some highly asymmetric functions.\n\n\nTarget function value to reach\nThe typical target function value for all functions is {f_\\mathrm{opt}}+{10^{-8}}. On many functions a value of {f_\\mathrm{opt}}+1 is not very difficult to reach, but the difficulty versus function value is not uniform for all functions. These properties are not intrinsic, that is {f_\\mathrm{opt}}+{10^{-8}} is not intrinsically “very good”. The value mainly reflects a scalar multiplier in the function definition."
  },
  {
    "objectID": "bbob/def.html#footnotes",
    "href": "bbob/def.html#footnotes",
    "title": "Function definitions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor our experimental setup see (Hansen et al. 2016, 2009) and for our performance assessment methodology see (Hansen et al. 2022).↩︎\nThe implementation provides an instance ID as input, such that a set of uniquely specified instances can be explicitly chosen.↩︎"
  }
]